[
["index.html", "The EMU-SDMS Manual Welcome", " The EMU-SDMS Manual Raphael Winkelmann Welcome Welcome to the EMU-SDMS Manual! Disclaimer: This manual is still in the making! It will eventually replace all the vignettes of emuR, wrassp as well as the EMU-webApp’s own documentation. This manual is intended to consolidate all of this information in one easy to find location that can easily be updated as new features are added. "],
["installing-the-emu-sdms.html", "1 Installing the EMU-SDMS 1.1 Version disclaimer 1.2 For developers and people interested in the source code", " 1 Installing the EMU-SDMS R Download the R programming language from https://cran.r-project.org/ Install the R programming language by executing the downloaded file and following the on-screen instructions. emuR Start up R. Enter install.packages(&quot;emuR&quot;) after the &gt; prompt to install the package. (You will only need to repeat this if package updates become available.) As the wrassp package is a dependency of the emuR package, it does not have to be installed separately. EMU-webApp (prerequisite) The only thing needed to use the EMU-webApp is a current HTML5 compatible browser (Chrome/Firefox/Safari/Opera/…). However, as most of the development and testing is done using Chrome we recommend using it, as it is by far the best tested browser. 1.1 Version disclaimer This document describes the following versions of the software components: wrassp Package version: 0.1.6 Git tag name: v0.1.6 (on master branch) emuR Package version: 1.0.0 Git tag name: v1.0.0 (on master branch) EMU-webApp Version: 0.1.12 Git SHA1: 7b044a9f9fe19f2eb6d03ec6ec3f20d5b1d25db2 As the development of the EMU Speech Database Management System is still ongoing, be sure you have the correct documentation to go with the version you are using. 1.2 For developers and people interested in the source code The information on how to install and/or access the source code of the developer version including the possibility of accessing the versions described in this document (via the Git tag names mentioned above) is given below. wrassp Source code is available here: https://github.com/IPS-LMU/wrassp/ Install developer version in R: install.packages(&quot;devtools&quot;); library(&quot;devtools&quot;); install_github(&quot;IPS-LMU/wrassp&quot;) Bug reports: https://github.com/IPS-LMU/wrassp/issues emuR Source code is available here: https://github.com/IPS-LMU/emuR/ Install developer version in R: install.packages(&quot;devtools&quot;); library(&quot;devtools&quot;); install_github(&quot;IPS-LMU/emuR&quot;) Bug reports: https://github.com/IPS-LMU/emuR/issues EMU-webApp Source code is available here: https://github.com/IPS-LMU/EMU-webApp/ Bug reports: https://github.com/IPS-LMU/EMU-webApp/issues "],
["chap-overview.html", "2 An overview of the EMU-SDMS1 2.1 The evolution of the EMU-SDMS 2.2 EMU-SDMS: System architecture and default workflow 2.3 EMU-SDMS: Is it something for you?", " 2 An overview of the EMU-SDMS1 The EMU Speech Database Management System (EMU-SDMS) is a collection of software tools which aims to be as close to an all-in-one solution for generating, manipulating, querying, analyzing and managing speech databases as possible. It was developed to fill the void in the landscape of software tools for the speech sciences by providing an integrated system that is centered around the R language and environment for statistical computing and graphics (R Core Team (2016)). This manual contains the documentation for the three software components wrassp, emuR and the EMU-webApp. In addition, it provides an in-depth description of the emuDB database format which is also considered an integral part of the new system. These four components comprise the EMU-SDMS and benefit the speech sciences and spoken language research by providing an integrated system to answer research questions such as: Given an annotated speech database, is the vowel height of the vowel @ (measured by its correlate, the first formant frequency) influenced by whether it appears in a strong or weak syllable? This manual is targeted at new EMU-SDMS users as well as users familiar with the legacy EMU system. In addition, it is aimed at people who are interested in the technical details such as data structures/formats and implementation strategies, be it for reimplementation purposes or simply for a better understanding of the inner workings of the new system. To accommodate these different target groups, after initially giving an overview of the system, this manual presents a usage tutorial that walks the user through the entire process of answering a research question. This tutorial will start with a set of .wav audio and Praat .TextGrid (Boersma and Weenink (2016)) annotation files and end with a statistical analysis to address the hypothesis posed by the research question. The following Part ?? of this documentation is separated into six chapters that give an in-depth explanation of the various components that comprise the EMU-SDMS and integral concepts of the new system. These chapters provide a tutorial-like overview by providing multiple examples. To give the reader a synopsis of the main functions and central objects that are provided by EMU-SDMS’s main R package emuR, an overview of these functions is presented in Part ??. Part ?? focuses on the actual implementation of the components and is geared towards people interested in the technical details. Further examples and file format descriptions are available in various appendices. This structure enables the novice EMU-SDMS user to simply skip the technical details and still get an in-depth overview of how to work with the new system and discover what it is capable of. A prerequisite that is presumed throughout this document is the reader’s familiarity with basic terminology in the speech sciences (e.g., familiarity with the international phonetic alphabet (IPA) and how speech is annotated at a coarse and fine grained level). Further, we assume the reader has a grasp of the basic concepts of the R language and environment for statistical computing and graphics. For readers new to R, there are multiple, freely available R tutorials online (e.g., https://en.wikibooks.org/wiki/Statistical_Analysis:_an_Introduction_using_R/R_basics). R also has a set of very detailed manuals and tutorials that come preinstalled with R. To be able to access R’s own “An Introduction to R” introduction, simply type help.start() into the R console and click on the link to the tutorial. 2.1 The evolution of the EMU-SDMS The EMU-SDMS has a number of predecessors that have been continuously developed over a number of years (e.g., Harrington et al. (1993), Cassidy and Harrington (1996), Cassidy and Harrington (2001), Bombien et al. (2006), Harrington (2010), John (2012)). The components presented here are the completely rewritten and newly designed, next incarnation of the EMU system, which we will refer to as the EMU Speech Database Management System (EMU-SDMS). The EMU-SDMS keeps most of the core concepts of the previous system, which we will refer to as the legacy system, in place while improving on things like usability, maintainability, scalability, stability, speed and more. We feel the redesign and reimplementation elevates the system into a modern set of speech and language tools that enables a workflow adapted to the challenges confronting speech scientists and the ever growing size of speech databases. The redesign has enabled us to implement several components of the new EMU-SDMS so that they can be used independently of the EMU-SDMS for tasks such as web-based collaborative annotation efforts and performing speech signal processing in a statistical programming environment. Nevertheless, the main goal of the redesign and reimplementation was to provide a modern set of tools that reduces the complexity of the tool chain needed to answer spoken language research questions down to a few interoperable tools. The tools the EMU-SDMS provides are designed to streamline the process of obtaining usable data, all from within an environment that can also be used to analyze, visualize and statistically evaluate the data. Upon developing the new system, rather than starting completely from scratch it seemed more appropriate to partially reuse the concepts of the legacy system in order to achieve our goals. A major observation at the time was that the R language and environment for statistical computing and graphics (R Core Team (2016)) was gaining more and more traction for statistical and data visualization purposes in the speech and spoken language research community. However, R was mostly only used towards the end of the data analysis chain where data usually was pre-converted into a comma-separated values or equivalent file format by the user using other tools to calculate, extract and pre-process the data. While designing the new EMU-SDMS, we brought R to the front of the tool chain to the point just beyond data acquisition. This allows the entire data annotation, data extraction and analysis process to be completed in R, while keeping the key user requirements in mind. Due to personal experiences gained by using the legacy system for research puposes and in various undergraduate courses (course material usually based on Harrington (2010)), we learned that the key user requirements were data and database portability, a simple installation process, a simplified/streamlined user experience and cross-platform availability. Supplying all of EMU-SDMS’s core functionality in the form of R packages that do not rely on external software at runtime seemed to meet all of these requirements. As the early incarnations of the legacy EMU system and its predecessors were conceived either at a time that predated the R system or during the infancy of R’s package ecosystem, the legacy system was implemented as a modular yet composite standalone program with a communication and data exchange interface to the R/Splus systems (see Cassidy and Harrington (2001) Section 3 for details). Recent developments in the package ecosystem of R such as the availability of the DBI package (R Special Interest Group on Databases (R-SIG-DB), Wickham, and Müller (2016)) and the related packages RSQLite and RPostgreSQL (Wickham, James, and Falcon (2014), Conway et al. (2016)), as well as the jsonlite package (Ooms (2014)) and the httpuv package (RStudio and Inc. (2015)), have made R an attractive sole target platform for the EMU-SDMS. These and other packages provide additional functional power that enabled the EMU-SDMS’s core functionality to be implemented in the form of R packages. The availability of certain R packages had a large impact on the architectural design decisions that we made for the new system. R Example shows the simple installation process which we were able to achieve due to the R package infrastructure. Compared to the legacy EMU and other systems, the installation process of the entire system has been reduced to a single R command. Throughout this documentation we will try to highlight how the EMU-SDMS is also able to meet the rest of the above key user requirements. rexample:overview-install # install the entire EMU-SDMS # by installing the emuR package install.packages(&quot;emuR&quot;) It is worth noting that throughout this manual R Example code snippets will be given in the form of R Example ??. These examples represent working R code that allow the reader to follow along in a hands-on manor and give a feel for what it is like working with the new EMU-SDMS. 2.2 EMU-SDMS: System architecture and default workflow As was previously mentioned, the new EMU-SDMS is made up of four main components. The components are the emuDB format; the R packages wrassp and emuR; and the web application, the EMU-webApp, which is EMU-SDMS’s new GUI component. An overview of the EMU-SDMS’s architecture and the components’ relationships within the system is shown in Figure 2.1. In Figure 2.1, the emuR package plays a central role as it is the only component that interacts with all of the other components of the EMU-SDMS. It performs file and DB handling for the files that comprise an emuDB (see Chapter @ref(chap:annot_struct_mod)); it uses the wrassp package for signal processing purposes (see Chapter 8; and it can serve emuDBs to the EMU-webApp (see Chapter 9). Figure 2.1: Schematic architecture of the EMU-SDMS Although the system is made of four main components, the user largely only interacts directly with the EMU-webApp and the emuR package. A summary of the default workflow illustrating theses interactions can be seen below: Load database into current R session (load_emuDB()). Database annotation / visual inspection (serve()). This opens up the EMU-webApp in the system’s default browser. Query database (query()). This is optionally followed by requery_hier() or requery_seq() as necessary (see Chapter 6 for details). Get trackdata (e.g. formant values) for the result of a query (get_trackdata()). Prepare data. Visually inspect data. Carry out further analysis and statistical processing. Initially the user creates a reference to an emuDB by loading it into their current R session using the load_emuDB() function (see step 1). This database reference can then be used to either serve (serve()) the database to the EMU-webApp or query (query()) the annotations of the emuDB (see steps 2 and 3). The result of a query can then be used to either perform one or more so-called requeries or extract signal values that correspond to the result of a query() or requery() (see step 4). Finally, the signal data can undergo further preparation (e.g., correction of outliers) and visual inspection before further analysis and statistical processing is carried out (see steps 5, 6 and 7). Although the R packages provided by the EMU-SDMS do provide functions for steps 4, 5 and 6, it is worth noting that the plethora of R packages that the R package ecosystem provides can and should be used to perform these duties. The resulting objects of most of the above functions are derived matrix or data.frame objects which can be used as inputs for hundreds if not thousands of other R functions. 2.3 EMU-SDMS: Is it something for you? Besides providing a fully integrated system, the EMU-SDMS has several unique features that set it apart from other current, widely used systems (e.g., Boersma and Weenink (2016), Wittenburg et al. (2006), Fromont and Hay (2012), Rose et al. (2006), McAuliffe and Sonderegger (2016)). To our knowledge, the EMU-SDMS is the only system that allows the user to model their annotation structures based on a hybrid model of time-based annotations (such as those offered by Praat’s tier-based annotation mechanics) and hierarchical timeless annotations. An example of such a hybrid annotation structure is displayed in Figure 2.2. These hybrid annotations benefit the user in multiple ways, as they reduce data redundancy and explicitly allow relationships to be expressed across annotation levels (see Chapter for further information on hierarchical annotations and Chapter on how to query these annotation structures). Figure 2.2: Example of a hybrid annotation combining time-based (Phonetic level) and hierarchical (Phoneme, Syllable, Text levels including the inter-level links) annotations. Further, to our knowledge, the EMU-SDMS is the first system that makes use of a web application as its primary GUI for annotating speech. This unique approach enables the GUI component to be used in multiple ways. It can be used as a stand-alone annotation tool, connected to a loaded emuDB via emuR’s serve() function and used to communicate to other servers. This enables it to be used as a collaborative annotation tool. An in-depth explanation of how this component can be used in these three scenarios is given in Chapter 9. As demonstrated in the default workflow of Section 2.2, an additional unique feature provided by EMU-SDMS is the ability to use the result of a query to extract derived (e.g., formants and RMS values) and complementary signals (e.g., electromagnetic articulography (EMA) data) that match the segments of a query. This, for example, aids the user in answering questions related to derived speech signals such as: Is the vowel height of the vowel @ (measured by its correlate, the first formant frequency) influenced by whether it appears in a strong or weak syllable?. Chapter gives a complete walk-through of how to go about answering this question using the tools provided by the EMU-SDMS. The features provided by the EMU-SDMS make it an all-in-one speech database management solution that is centered around R. It enriches the R platform by providing specialized speech signal processing, speech database management, data extraction and speech annotation capabilities. By achieving this without relying on any external software sources except the web browser, the EMU-SDMS significantly reduces the number of tools the speech and spoken language researcher has to deal with and helps to simplify answering research questions. As the only prerequisite for using the EMU-SDMS is a basic familiarity with the R platform, if the above features would improve your workflow, the EMU-SDMS is indeed for you. References "],
["chap-tutorial.html", "3 A tutorial on how to use the EMU-SDMS2 3.1 Converting the TextGrid collection 3.2 Loading and inspecting the database 3.3 Querying and autobuilding the annotation structure 3.4 Autobuilding 3.5 Signal extraction and exploration 3.6 Vowel height as a function of word types (content vs. function): evaluation and statistical analysis 3.7 Conclusion", " 3 A tutorial on how to use the EMU-SDMS2 Using the tools provided by the EMU-SDMS, this tutorial chapter gives a practical step-by-step guide to answering the question: Given an annotated speech database, is the vowel height of the vowel @ (measured by its correlate, the first formant frequency) influenced by whether it appears in a content or function word? The tutorial only skims over many of the concepts and functions provided by the EMU-SDMS. In-depth explanations of the various functionalities are given in later chapters of this documentation. As the EMU-SDMS is not concerned with the raw data acquisition, other tools such as SpeechRecorder by Draxler and Jänsch (2004) are first used to record speech. However, once audio speech recordings are available, the system provides multiple conversion routines for converting existing collections of files to the new emuDB format described in Chapter 5 and importing them into the new EMU system. The current import routines provided by the emuR package are: convert_TextGridCollection() - Convert TextGrid collections (.wav and .TextGrid files) to the emuDB format, convert_BPFCollection() - Convert Bas Partitur Format (BPF) collections (.wav and .par files) to the emuDB format, convert_txtCollection() - Convert plain text file collections format (.wav and .txt files) to the emuDB format, convert_legacyEmuDB() - Convert the legacy EMU database format to the emuDB format and create_emuDB() followed by add_link/levelDefinition and import_mediaFiles() - Creating emuDBs from scratch with only audio files present. The emuR package comes with a set of example files and small databases that are used throughout the emuR documentation, including the functions help pages. These can be accessed by typing help(functionName) or the short form ?functionName. R Example ?? illustrates how to create this demo data in a user-specified directory. Throughout the examples of this documentation the directory that is provided by the base R function tempdir() will be used, as this is available on every platform supported by R (see ?tempdir for further details). As can be inferred from the list.dirs() output in R Example ??, the emuR_demoData directory contains a separate directory containing example data for each of the import routines. Additionally, it contains a directory containing an emuDB called ae (the directories name is ae_emuDB, where _emuDB is the default suffix given to directories containing a emuDB; see Chapter 5). rexample:tutorial-create-emuRdemoData # load the package library(emuR) # create demo data in directory provided by the tempdir() function # (of course other directory paths may be chosen) create_emuRdemoData(dir = tempdir()) # create path to demo data directory, which is # called &quot;emuR_demoData&quot; demoDataDir = file.path(tempdir(), &quot;emuR_demoData&quot;) # show demo data directories list.dirs(demoDataDir, recursive = F, full.names = F) ## [1] &quot;ae_emuDB&quot; &quot;BPF_collection&quot; &quot;legacy_ae&quot; ## [4] &quot;TextGrid_collection&quot; &quot;txt_collection&quot; This tutorial will start by converting a TextGrid collection containing seven annotated single-sentence utterances of a single male speaker to the emuDB format3. In the EMU-SDMS, a file collection such as a TextGrid collection refers to a set of file pairs where two types of files with different file extentions are present (e.g., .ext1 and .ext2). It is vital that file pairs have the same basenames (e.g., A.ext1 and A.ext2 where A represents the basename) in order for the conversion functions to be able to pair up files that belong together. As other speech software tools also encourage such file pairs (e.g., Kisler et al. (2015)) this is a common collection format in the speech sciences. R Example shows such a file collection that is part of emuR’s demo data. Figure @ref(fig:msajc003_praatTG) shows the content of an annotation as displayed by Praat’s &quot;Draw visible sound and Textgrid...&quot; procedure. rexample:showTGcolContent # create path to TextGrid collection tgColDir = file.path(demoDataDir, &quot;TextGrid_collection&quot;) # show content of TextGrid_collection directory list.files(tgColDir) ## [1] &quot;msajc003.TextGrid&quot; &quot;msajc003.wav&quot; &quot;msajc010.TextGrid&quot; ## [4] &quot;msajc010.wav&quot; &quot;msajc012.TextGrid&quot; &quot;msajc012.wav&quot; ## [7] &quot;msajc015.TextGrid&quot; &quot;msajc015.wav&quot; &quot;msajc022.TextGrid&quot; ## [10] &quot;msajc022.wav&quot; &quot;msajc023.TextGrid&quot; &quot;msajc023.wav&quot; ## [13] &quot;msajc057.TextGrid&quot; &quot;msajc057.wav&quot; (#fig:msajc003_praatTG)TextGrid annotation of the emuR_demoData/TextGrid_collection/msajc003.wav / .TextGrid file pair containing the tiers (from top to bottom): Utterance, Intonational, Intermediate, Word, Accent, Text, Syllable, Phoneme, Phonetic, Tone, Foot. 3.1 Converting the TextGrid collection The convert_TextGridCollection() function converts a TextGrid collection to the emuDB format. A precondition that all .TextGrid files have to fulfill is that they must all contain the same tiers. If this is not the case, yet there is an equal tier subset that is contained in all the TextGrid files, this equal subset may be chosen. For example, if all .TextGrid files contain only the tier Phonetic: IntervalTier the conversion will work. However, if a single .TextGrid of the collection has the additional tier Tone: TextTier the conversion will fail. In this case the conversion could be made to work by specifying the equal subset (e.g., equalSubset = c(&quot;Phonetic&quot;)) and passing it on to the tierNames function argument convert_TextGridCollection(..., tierNames = equalSubset, ...). As can be seen in Figure ??, the TextGrid files provided by the demo data contain eleven tiers. To reduce the complexity of the annotations for this tutorial we will only convert the tiers Word (content: C vs. function: F word annotations), Syllable (strong: S vs. weak: W syllable annotations), Phoneme (phoneme level annotations) and Phonetic (phonetic annotations using Speech Assessment Methods Phonetic Alphabet (SAMPA) symbols - Wells and others (1997)) using the tierNames parameter. This conversion can be seen in R Example @ref(rexample:tutorial_tgconv). rexample:tutorial_tgconv # convert TextGrid collection to the emuDB format convert_TextGridCollection(dir = tgColDir, dbName = &quot;myFirst&quot;, targetDir = tempdir(), tierNames = c(&quot;Word&quot;, &quot;Syllable&quot;, &quot;Phoneme&quot;, &quot;Phonetic&quot;)) The above call to convert_TextGridCollection() creates a new emuDB directory in the tempdir() directory called myFirst_emuDB. This emuDB contains annotation files that contain the same Word, Syllable, Phoneme and Phonetic segment tiers as the original .TextGrid files as well as copies of the original (.wav) audio files. For further details about the structure of an emuDB, see Chapter 5 of this document. 3.2 Loading and inspecting the database As mentioned in Section @ref(sec:overview_sysArch), the first step when working with an emuDB is to load it into the current R session. R Example ?? shows how to load the converted TextGrid collection into R using the load_emuDB() function. rexample:tutorial-loadEmuDB # get path to emuDB called &quot;myFirst&quot; # that was created by convert_TextGridCollection() path2directory = file.path(tempdir(), &quot;myFirst_emuDB&quot;) # load emuDB into current R session dbHandle = load_emuDB(path2directory, verbose = FALSE) 3.2.1 Overview Now the myFirst emuDB is loaded into R, an overview of the current status and configuration of the database can be displayed using the summary() function as shown in R Example @ref(rexample:tutorial_summary). rexample:tutorial_summary # show summary summary(dbHandle) ## Name: myFirst ## UUID: 834e47f4-3e00-44e0-bf90-5967b0d08c84 ## Directory: /private/var/folders/yk/8z9tn7kx6hbcg_9n4c1sld980000gn/T/RtmpT2PHGx/myFirst_emuDB ## Session count: 1 ## Bundle count: 7 ## Annotation item count: 664 ## Label count: 664 ## Link count: 0 ## ## Database configuration: ## ## SSFF track definitions: ## NULL ## ## Level definitions: ## name type nrOfAttrDefs attrDefNames ## 1 Word SEGMENT 1 Word; ## 2 Syllable SEGMENT 1 Syllable; ## 3 Phoneme SEGMENT 1 Phoneme; ## 4 Phonetic SEGMENT 1 Phonetic; ## ## Link definitions: ## NULL The extensive output of summary() is split into a top and bottom half, where the top half focuses on general information about the database (name, directory, annotation item count, etc.) and the bottom half displays information about the various SSFF track, level and link definitions of the emuDB. The summary information about the level definitions shows, for instance, that the myFirst database has a Word level of type SEGMENT and therefore contains annotation items that have a start time and a segment duration. It is worth noting that information about the SSFF track, level and link definitions corresponds to the output of the list_ssffTrackDefinitions(), list_levelDefinitions() and list\\_linkDefinitions() functions. 3.2.2 Database annotation and visual inspection The EMU-SDMS has a unique approach to annotating and visually inspecting databases, as it utilizes a web application called the EMU-webApp to act as its GUI. To be able to communicate with the web application the emuR package provides the serve() function which is used in R Example @ref(rexample:tutorial_serve). rexample:tutorial_serve # serve myFirst emuDB to the EMU-webApp serve(dbHandle) Executing this command will block the R console, automatically open up the system’s default browser and display the following message in the R console: ## Navigate your browser to the EMU-webApp URL: ## http://ips-lmu.github.io/EMU-webApp/ (should happen autom... ## Server connection URL: ## ws://localhost:17890 ## To stop the server press the &#39;clear&#39; button in the ## EMU-webApp or close/reload the webApp in your browser. The EMU-webApp, which is now connected to the database via the serve() function, can be used to visually inspect and annotate the emuDB. Figure 3.1 displays a screenshot of what the EMU-webApp looks like after automatically connecting to the server. As the EMU-webApp is a very feature-rich software annotation tool, this documentation has a whole chapter (see Chapter 9) on how to use it, what it is capable of and how to configure it. Further, the web application provides its own documentation which can be accessed by clicking the EMU icon in the top right hand corner of the application’s top menu bar. To close the connection and free up the blocked R console, simply click the clear button in the top menu bar of the EMU-webApp. Figure 3.1: Screenshot of EMU-webApp displaying msajc003 bundle of myFirst emuDB. 3.3 Querying and autobuilding the annotation structure An integral step in the default workflow of the EMU-SDMS is querying the annotations of a database. The emuR package implements a query() function to accomplish this task. This function evaluates an EMU Query Language (EQL) expression and extracts the annotation items from the database that match a query expression. As Chapter 6 gives a detailed description of the query mechanics provided by emuR, this tutorial will only use a very small, hopefully easy to understand subset of the EQL. The output of the summary() command in R Example @ref(rexample:tutorial_summary) and the screenshot in Figure 3.1 show that the myFirst emuDB contains four levels of annotations. R Example ?? shows four separate queries that query various segments on each of the available levels. The query expressions all use the matching operator == which returns annotation items whose labels match those specified to the right of the operator and that belong to the level specified to the left of the operator (i.e., LEVEL == LABEL; see Chapter 6 for a detailed description). rexample:tutorial-simpleQuery # query all segments containing the label # &quot;C&quot; (== content word) of the &quot;Word&quot; level sl_text = query(emuDBhandle = dbHandle, query = &quot;Word == C&quot;) # query all segments containing the label # &quot;S&quot; (== strong syllable) of the &quot;Syllable&quot; level sl_syl = query(emuDBhandle = dbHandle, query = &quot;Syllable == S&quot;) # query all segments containing the label # &quot;f&quot; on the &quot;Phoneme&quot; level sl_phoneme = query(dbHandle, query = &quot;Phoneme == f&quot;) # query all segments containing the label # &quot;n&quot; of the &quot;Phonetic&quot; level sl_phonetic = query(dbHandle, query = &quot;Phonetic == n&quot;) # show class vector of query result class(sl_phonetic) ## [1] &quot;emuRsegs&quot; &quot;emusegs&quot; &quot;data.frame&quot; # show first entry of sl_phonetic head(sl_phonetic, n = 1) ## segment list from database: myFirst ## query was: Phonetic == n ## labels start end session bundle level type ## 1 n 1031.925 1195.925 0000 msajc003 Phonetic SEGMENT # show summary of sl_phonetic summary(sl_phonetic) ## segment list from database: myFirst ## query was: Phonetic == n ## with 12 segments ## ## Segment distribution: ## ## n ## 12 As demonstrated in R Example @ref(rexample:tutorial_simpleQuery), the result of a query is an emuRsegs object, which is a super-class of the common data.frame. This object is often referred to as a segment list, or “seglist”. A segment list carries information about the extracted annotation items such as the extracted labels, the start and end times of the segments, the sessions and bundles the items are from and the levels they belong to. An in-depth description of the information contained in a segment list is given in Section ??. R Example @ref(rexample:tutorial_simpleQuery) shows that the summary() function can also be applied to a segment list object to get an overview of what is contained within it. This can be especially useful when dealing with larger segment lists. 3.4 Autobuilding The simple queries illustrated above query segments from a single level that match a certain label. However, the EMU-SDMS offers a mechanism for performing inter-level queries such as: Query all Phonetic items that contain the label “n” and are part of a content word. For such queries to be possible, the EMU-SDMS offers very sophisticated annotation structure modeling capabilities, which are described in Chapter ??. For the sake of this tutorial we will focus on converting the flat segment level annotation structure displayed in Figure 3.1 to a hierarchical form as displayed in Figure 3.2, where only the Phonetic level carries time information and the annotation items on the other levels are explicitly linked to each other to form a hierarchical annotation structure. Figure 3.2: Example of a hierarchical annotation of the content (==C) word violently belonging to the msajc012 bundle of the myFirst demo emuDB. As it is a very laborious task to manually link annotation items together using the EMU-webApp and the hierarchical information is already implicitly contained in the time information of the segments and events of each level, we will now use a function provided by the emuR package to build these hierarchical structures using this information called autobuild_linkFromTimes(). R Example ?? shows the calls to this function which autobuild the hierarchical annotations in the myFirst database. As a general rule for autobuilding hierarchical annotation structures, a good strategy is to start the autobuilding process beginning with coarser grained annotation levels (i.e., the Word/Syllable level pair in our example) and work down to finer grained annotations (i.e., the Syllable/Phoneme and Phoneme/Phonetic} level pairs in our example). To build hierachical annotation structures we need link definitions, which together with the level definitions define the annotation structure for the entire database (see Chapter ?? for further details). The autobuild_linkFromTimes() calls in R Example ?? use the newLinkDefType parameter, which if defined automatically adds a link definition to the database. rexample:tutorial-autobuild # invoke autobuild function # for &quot;Word&quot; and &quot;Syllable&quot; levels autobuild_linkFromTimes(dbHandle, superlevelName = &quot;Word&quot;, sublevelName = &quot;Syllable&quot;, convertSuperlevel = TRUE, newLinkDefType = &quot;ONE_TO_MANY&quot;) # invoke autobuild function # for &quot;Syllable&quot; and &quot;Phoneme&quot; levels autobuild_linkFromTimes(dbHandle, superlevelName = &quot;Syllable&quot;, sublevelName = &quot;Phoneme&quot;, convertSuperlevel = TRUE, newLinkDefType = &quot;ONE_TO_MANY&quot;) # invoke autobuild function # for &quot;Phoneme&quot; and &quot;Phonetic&quot; levels autobuild_linkFromTimes(dbHandle, superlevelName = &quot;Phoneme&quot;, sublevelName = &quot;Phonetic&quot;, convertSuperlevel = TRUE, newLinkDefType = &quot;MANY_TO_MANY&quot;) Figure 3.3: Schematic annotation structure of the emuDB after calling the autobuild function in R Example (???)(rexample:tutorial-autobuild). As the autobuild_linkFromTimes() function automatically creates backup levels to avoid the accidental loss of boundary or event time information, R Example @ref(rexample:tutorial_delBackupLevels) shows how these backup levels can be removed to clean up the database. However, using the remove_levelDefinition() function with its force parameter set to TRUE is a very invasive action. Usually this would not be recommended, but for this tutorial we are keeping everything as clean as possible. rexample:tutorial-delBackupLevels # list level definitions # as this reveals the &quot;-autobuildBackup&quot; levels # added by the autobuild_linkFromTimes() calls list_levelDefinitions(dbHandle) ## name type nrOfAttrDefs attrDefNames ## 1 Word ITEM 1 Word; ## 2 Syllable ITEM 1 Syllable; ## 3 Phoneme ITEM 1 Phoneme; ## 4 Phonetic SEGMENT 1 Phonetic; ## 5 Word-autobuildBackup SEGMENT 1 Word-autobuildBackup; ## 6 Syllable-autobuildBackup SEGMENT 1 Syllable-autobuildBackup; ## 7 Phoneme-autobuildBackup SEGMENT 1 Phoneme-autobuildBackup; # remove the levels containing the &quot;-autobuildBackup&quot; # suffix remove_levelDefinition(dbHandle, name = &quot;Word-autobuildBackup&quot;, force = TRUE, verbose = FALSE) remove_levelDefinition(dbHandle, name = &quot;Syllable-autobuildBackup&quot;, force = TRUE, verbose = FALSE) remove_levelDefinition(dbHandle, name = &quot;Phoneme-autobuildBackup&quot;, force = TRUE, verbose = FALSE) # list level definitions list_levelDefinitions(dbHandle) ## name type nrOfAttrDefs attrDefNames ## 1 Word ITEM 1 Word; ## 2 Syllable ITEM 1 Syllable; ## 3 Phoneme ITEM 1 Phoneme; ## 4 Phonetic SEGMENT 1 Phonetic; # list level definitions # which were added by the autobuild functions list_linkDefinitions(dbHandle) ## type superlevelName sublevelName ## 1 ONE_TO_MANY Word Syllable ## 2 ONE_TO_MANY Syllable Phoneme ## 3 MANY_TO_MANY Phoneme Phonetic As can be seen by the output of list_levelDefinitions() and list_linkDefinitions() in R Example ??, the annotation structure of the myFirst emuDB now matches that displayed in Figure @ref(fig:tutorial_simpleAnnotStruct). Using the serve() function to open the emuDB in the EMU-webApp followed by clicking on the show hierarchy button in the top menu (and rotating the hierarchy by 90 degrees by clicking the rotate by 90 degrees button) will result in a view similar to the screenshot of Figure ??. Figure 3.4: Screenshot of EMU-webApp displaying the autobuilt hierarchy of the myFirst emuDB. 3.4.1 Querying the hierarchical annotations Having this hierarchical annotation structure now allows us to formulate a query that helps answer the originally stated question: Given an annotated speech database, is the vowel height of the vowel @ (measured by its correlate, the first formant frequency) influenced by whether it appears in a content or function word?. R Example shows how all the @ vowels in the myFirst database are queried. rexample:tutorial-labelGroupQuery # query annotation items containing # the labels @ on the Phonetic level sl_vowels = query(dbHandle, &quot;Phonetic == @&quot;) # show first entry of sl_vowels head(sl_vowels, n = 1) ## segment list from database: myFirst ## query was: Phonetic == @ ## labels start end session bundle level type ## 1 @ 1506.175 1548.425 0000 msajc003 Phonetic SEGMENT As the type of word (content vs. function) for each @ vowel that was just extracted is also needed, we can use the requery functionality of the EMU-SDMS (see Chapter 6) to retrieve the word type for each @ vowel. A requery essentially moves through a hierarchical annotation (vertically or horizontally) starting from the segments that are passed into the requery function. R Example ?? illustrates the usage of the hierarchical requery function, requery_hier(), to retrieve the appropriate annotation items from the *Word level. rexample:tutorial-requery # hierarchical requery starting from the items in sl_vowels # and moving up to the &quot;Word&quot; level sl_wordType = requery_hier(dbHandle, seglist = sl_vowels, level = &quot;Word&quot;, calcTimes = FALSE) # show first entry of sl_wordType head(sl_wordType, n = 1) ## segment list from database: myFirst ## query was: FROM REQUERY ## labels start end session bundle level type ## 1 F NA NA 0000 msajc003 Word ITEM # show that sl_vowel and sl_wordType have the # same number of row entries nrow(sl_vowels) == nrow(sl_wordType) ## [1] TRUE As can be seen by the nrow() comparison in R Example ??, the segment list returned by the requery_hier() function has the same number of rows as the original sl_vowels segment list. This is important, as each row of both segment lists line up and allow us to infer which segment belongs to which word type (e.g., vowel sl_vowels[5,] belongs to the word type sl_wordType[5,]). 3.5 Signal extraction and exploration Now that the vowel and word type information including the vowel start and end time information has been extracted from the database, this information can be used to extract signal data that matches these segments. Using the emuR function get_trackdata() we can calculate the formant values in real time using the formant estimation function, forest(), provided by the wrassp package (see Chapter 8 for details). R Example ?? shows the usage of this function. rexample:tutorial-getTrackdata # get formant values for the vowel segments td_vowels = get_trackdata(dbHandle, seglist = sl_vowels, onTheFlyFunctionName = &quot;forest&quot;, verbose = F) # show class vector class(td_vowels) ## [1] &quot;trackdata&quot; # show dimensions dim(td_vowels) ## [1] 28 4 # display all values for fifth segment td_vowels[5,] ## trackdata from track: fm ## index: ## left right ## 1 12 ## ftime: ## start end ## [1,] 2447.5 2502.5 ## data: ## T1 T2 T3 T4 ## 2447.5 303 1031 2266 3366 ## 2452.5 289 967 2250 3413 ## 2457.5 296 905 2273 3503 ## 2462.5 321 885 2357 3506 ## 2467.5 316 889 2397 3475 ## 2472.5 306 863 2348 3548 ## 2477.5 314 832 2339 3611 ## 2482.5 325 795 2342 3622 ## 2487.5 339 760 2322 3681 ## 2492.5 335 746 2316 3665 ## 2497.5 341 734 2306 3688 ## 2502.5 361 733 2304 3692 As can be seen by the call to the class() function, the resulting object is of the type trackdata and has 28 entries. This corresponds to the number of rows contained in the segment lists extracted above (i.e., nrow(sl_vowels)). This indicates that this object contains data for each of the segments that correspond to each of the row entries of the segment lists (i.e., td_vowels[5,] are the formant values belonging to sl_vowels[5,]). As the columns T1, T2, T3, T4 of the printed output of td_vowels[5,] suggest, the forest function estimates four formant values. We will only be concerned with the first (column T1) and second (column T2). R Example ?? shows a call to emuR’s dplot() function which produces the plot displayed in Figure 3.5. The first call to the dplot() function plots all 28 first formant trajectories (achieved by indexing the first column i.e., T1: x = td_vowels[, 1]). To clean up the cluttered left plot, the second call to the dplot() function additionally uses the average parameter to plot only the ensemble averages of all @ vowels and time-normalizes the trajectories (normalise = TRUE) to an interval between 0 and 1. rexample:tutorial-dplot # two plots next to each other formantNr = 1 par(mfrow = c(1,2)) dplot(x = td_vowels[, formantNr], labs = sl_vowels$labels, xlab = &quot;Duration (ms)&quot;, ylab = paste0(&quot;F&quot;, formantNr, &quot; (Hz)&quot;)) dplot(x = td_vowels[, 1], labs = sl_vowels$labels, normalise = TRUE, average = TRUE, xlab = &quot;Normalized time&quot;, ylab = paste0(&quot;F&quot;, formantNr, &quot; (Hz)&quot;)) # back to single plot par(mfrow = c(1,1)) Figure 3.5: dplot() plots of F1 trajectories. The left plot displays all trajectories while the right plot displays the ensemble average of all @ vowels. Figure 3.5 gives an overview of the first formant trajectories of the @ vowels. For the purpose of data exploration and to get an idea of where the individual vowel classes lie on the F2 x F1 plane, which indirectly provides information about vowel height and tongue position, R Example ?? makes use of the eplot() function. This produces Figure 3.6. To be able to use the eplot() function, the td_vowels object first has to be modified, as it contains entire formant trajectories but two dimensional data is needed to be able to display it on the F2 x F1 plain. This can, for example, be achieved by only extracting temporal mid-point formant values for each vowel using the get_trackdata() function utilizing its cut parameter. R Example ?? shows an alternative approach using the dcut() function to essentially cut the formant trajectories to a specified proportional segment. By using only the left.time = 0.5 (and not specifying right.time) only the formant values that are closest to the temporal mid-point are cut from the trajectories. rexample:tutorial-eplot # cut formant trajectories at temporal mid-point td_vowels_midpoint = dcut(td_vowels, left.time = 0.5, prop = TRUE) # show dimensions of td_vowels_midpoint dim(td_vowels_midpoint) # generate plot eplot(x = td_vowels_midpoint[,1:2], labs = sl_vowels$labels, dopoints = TRUE, formant = TRUE, xlab=&quot;F2 (Hz)&quot;, ylab = &quot;F1 (Hz)&quot; ) Figure 3.6: 95% ellipses for F2 x F1 data extracted from the temporal midpoint of the vowel segments. Figure @ref{fig:tutorial-eplot} displays the first two formants extracted at the temporal midpoint of every @ vowel in sl_vowels. These formants are plotted on the F2 x F1 plane, and their 95% ellipsis distribution is also shown. Although not necessarily applicable to the question posed at the beginning of this tutorial, the data exploration using the dplot() and eplot() functions can be very helpful tools for providing an overview of the data at hand. 3.6 Vowel height as a function of word types (content vs. function): evaluation and statistical analysis The above data exploration only dealt with the actual @ vowels and disregarded the syllable type they occurred in. However, the question in the introduction of this chapter focuses on whether the @ vowel occurs in a content (labeled C) or function (labeled F) word. For data inspection purposes, R Example ?? initially extracts the central 60% (left.time = 0.2 and right.time = 0.8) of the formant trajectories from td_vowels using dcut() and displays them using dplot(). It should be noted that the call to dplot() uses the labels of the sl_wordType object as opposed to those of sl_vowels. This causes the dplot() functions to group the trajectories by their word type as opposed to their vowel labels as displayed in Figure @ref(fig:tutorial_dplotSylTyp). rexample:tutorial-dplotSylTyp # extract central 60% from formant trajectories td_vowelsMidSec = dcut(td_vowels, left.time = 0.2, right.time = 0.8, prop = TRUE) # plot first formant trajectories formantNr = 1 dplot(x = td_vowelsMidSec[, formantNr], labs = sl_wordType$labels, normalise = TRUE, average = TRUE, xlab = &quot;Normalized time&quot;, ylab = paste(&quot;F&quot;, formantNr, &quot; (Hz)&quot;)) Figure 3.7: Ensemble averages of F1 contours of all tokens of the central 60% of vowels grouped by word type (function (F) vs. content (W)). As can be seen in Figure 3.7, there seems to be a distinction in F1 trajectory height between vowels in content and function words. R Example ?? shows the code to produce a boxplot using the ggplot2 package to further visually inspect the data (see Figure 3.8 for the plot produced by R Example @ref{rexample:tutorial-boxplot}). rexample:tutorial-boxplot formantNr = 1 # use trapply to calculate the means of the 60% # formant trajectories td_vowelsMidSec_mean = trapply(td_vowelsMidSec[, formantNr], fun = mean, simplify = T) # create new data frame that contains the mean # values and the corresponding labels df = data.frame(wordType = sl_wordType$labels, meanF1 = td_vowelsMidSec_mean) # load library library(ggplot2) # create boxplot using ggplot ggplot(df, aes(wordType, meanF1)) + geom_boxplot() + labs(x = &quot;Word type&quot;, y = paste0(&quot;mean F&quot;, formantNr, &quot; (Hz)&quot;)) Figure 3.8: Boxplot produced using ggplot2 to visualize the difference in F1 depending on whether the vowel occurs in content (C) or function (F) word. To confirm or reject this, R Example ?? presents a very simple statistical analysis of the F1 mean values of the 60% mid-section formant trajectories.4 First, a Shapiro-Wilk test for normality of the distributions of the F1 means for both word types is carried out. As only one type is normally distributed, a Wilcoxon rank sum test is performed. The density distributions (commented out plot() function calls in R Example ??) are displayed in Figure @ref(fig:tutorial_stats1). rexample:tutorial-stats1 # calculate density for vowels in function words distrF = density(df[df$wordType == &quot;F&quot;,]$meanF1) # uncomment to visualize distribution # plot(distrF) # check that vowels in function # words are normally distributed shapiro.test(df[df$wordType == &quot;F&quot;,]$meanF1) ## ## Shapiro-Wilk normality test ## ## data: df[df$wordType == &quot;F&quot;, ]$meanF1 ## W = 0.98687, p-value = 0.9887 # p-value &gt; 0.05 implying that the distribution # of the data ARE NOT significantly different from # normal distribution -&gt; we CAN assume normality # calculate density for vowels in content words distrC = density(df[df$wordType == &quot;C&quot;,]$meanF1) # uncomment to visualize distribution # plot(distrC) # check that vowels in content # words are normally distributed: shapiro.test(df[df$wordType == &quot;C&quot;,]$meanF1) ## ## Shapiro-Wilk normality test ## ## data: df[df$wordType == &quot;C&quot;, ]$meanF1 ## W = 0.66506, p-value = 1.506e-05 # p-value &lt; 0.05 implying that the distribution # of the data ARE significantly different from # normal distribution -&gt; we CAN NOT assume normality # (this somewhat unexpected result is probably # due to the small sample size used in this toy example) # -&gt; use Wilcoxon rank sum test # perform Wilcoxon rank sum test to establish # whether vowel F1 depends on word type wilcox.test(meanF1 ~ wordType, data = df) ## ## Wilcoxon rank sum test ## ## data: meanF1 by wordType ## W = 121, p-value = 0.03752 ## alternative hypothesis: true location shift is not equal to 0 Figure 3.9: Plots of density distributions of vowels in content words (left plot) and vowels in function words (right plot) in R Example (???)(rexample:tutorial_stats1). As shown by the result of wilcox.test() in R Example @ref(rexample:tutorial_stats1), word type (C vs. F) has a significant influence on the vowel’s F1 (W=121, p&lt;0.05). Hence, the answer to the initially proposed question: Given an annotated speech database, is vowel height of the vowel @ (measured by its correlate, the first formant frequency) influenced by whether it appears in a content or function word? is yes! 3.7 Conclusion The tutorial given in this chapter gave an overview of what it is like working with the EMU-SDMS to try to solve a research question. As many of the concepts were only briefly explained, it is worth noting that explicit explanations of the various components and integral concepts are given in following chapters. Further, additional use cases that have been taken from the emuR_intro vignette can be found in Appendix @ref(app_chap:useCases). These use cases act as templates for various types of research questions and will hopefully aid the user in finding a solution similar to what she or he wishes to achieve. References "],
["chap-annot-struct-mod.html", "4 Annotation Structure Modeling5 4.1 Per database annotation structure definition 4.2 Parallel labels and multiple attributes 4.3 Metadata strategy using single bundle root nodes 4.4 Conclusion", " 4 Annotation Structure Modeling5 The EMU-SDMS facilitates annotation structure modeling that surpasses that available in many other commonly used systems. This chapter provides an in-depth explanation of the annotation structure modeling capabilities the EMU-SDMS offers. One of the most common approaches for creating time-aligned annotations has been to differentiate between events that occur at a specific point in time but have no duration and segments that start at a point in time and have a duration. These annotation items are then grouped into time-ordered sets that are often referred to as tiers. As certain research questions benefit from different granularities of annotation, the timeline is often used to relate implicitly items from multiple tiers to each other as shown in Figure 4.1A. While sufficient for single or unrelated tier annotations, we feel this type of representation is not suitable for more complex annotation structures, as it results in unnecessary, redundant data and data sets that are often difficult to analyze. This is because there are no explicit relationships between annotation items, and it is often necessary to introduce error tolerance values to analyze slightly misaligned time values to find relationships iteratively over multiple levels. The main reason for the prevalence of this sub-optimal strategy is largely because the available software tools (e.g., Praat by Boersma and Weenink (2016)) do not permit any other forms of annotations. These widely used annotation tools often only permit the creation and manipulation of segment and event tiers which in turn has forced users to model their annotation structures on these building blocks alone. Linguists who deal with speech and language on a purely symbolic level tend to be more familiar with a different type of annotation structure modeling. They often model their structures in the form of a vertically oriented, directed acyclic graph that, but for a few exceptions that are needed for things like elision modeling (e.g., the /ɪ/ elision that may occur between the canonical representation of the word family /fæmɪli/ and its phonetic representation [fæmli]), loosely adheres to the formal definition of a tree in the graph-theoretical sense (Knuth (1968)) as depicted in Figure @ref(fig:annot_structhybridAnnot)B. While this form of modeling explicitly defines relationships between annotation items (represented by dashed lines in Figure @ref(fig:annot_structhybridAnnot)B), it lacks the ability to map these items to the timeline and therefore the matching speech signal. Figure 4.1: A: a purely time-aligned annotation; B: a purely timeless, symbolic annotation; C: a time-aligned hierarchical annotation. To our knowledge, the legacy EMU system (Cassidy and Harrington (2001)) and its predecessors (e.g., Harrington et al. (1993)) were the first to fuse pragmatically purely time-aligned and symbolic tree-like annotations. This was achieved by providing software tools that allowed for these types of annotation structures to be generated, queried and evaluated. In practice, each annotation item had its own unique identifier within the annotation. These unique IDs could then be used to reference each individual item and link them together using dominance relations to form the hierarchical annotation structure. On the one hand, this dominance relation implies the temporal inclusion of the linked sub-level items and was partially predicated on the no-crossing constraint as described in Coleman and Local (1991)). This constraint does not permit the crossing of dominance relationships with respect to their sequential ordering (see also Section 4.2 of Cassidy and Harrington (2001)). Since the dominance relations imply temporal inclusion, events can only be children in a parent-child relationship. To allow for timeless annotation items, a further timeless level type was used to complement the segment and event type levels used for time-aligned annotations. Each level of annotation items was stored as an ordered set to ensure the sequential integrity of both the time-aligned and timeless item levels. The legacy system also reduced data redundancy by allowing parallel annotations to be defined in the form of linearly linked levels for any given level (e.g., a segment level bearing SAMPA annotations as well as IPA UTF-8 annotations). The new EMU-SDMS has adopted some concepts of the legacy system in that levels of type SEGMENT and EVENT contain annotation items with labels and time information, similar to the tiers known from other software tools such as Praat, while levels of type ITEM are timeless and contain annotation items with labels only. SEGMENT and EVENT levels differ in that units at the SEGMENTs level have a start time and a duration, while units at the EVENT level contain a single time point only. Additionally, every annotation item is able to contain multiple labels and has a unique identifier which is used to link items across levels. These building blocks provide the user with a general purpose annotation modeling tool that allows complex annotation structures to be modeled that best represent the data. An example of a time-aligned hierarchical annotation is depicted in Figure @ref(fig:annot_structhybridAnnot)C, which essentially combines the annotation of Figure @ref(fig:annot_structhybridAnnot)B with the most granular time-bearing level (i.e. the ``Phonetic’’ level) of Figure @ref(fig:annot_structhybridAnnot)A. In accordance with other approaches (among others see Bird and Liberman (2001), Zipser and Romary (2010), Ide and Romary (2004)), the EMU-SDMS annotation structure can be viewed as a graph that consists of three types of nodes (EVENTs, SEGMENTs, ITEMs) and two types of relations (dominance and sequence) which are directed, transitive and indicate the dominance and sequential relationships between nodes of the graph. As was shown in a pseudo-code example that converted an Annotation Graph (Bird and Liberman (2001)) into the legacy EMU annotation format in Cassidy and Harrington (2001), these formats can be viewed as conceptually equivalent sub- or super-set representations of each other. This has also been shown by developments of meta models with independent data representation such as Salt (Zipser and Romary (2010)), which enable abstract internal representations to be derived that can be exported to equal-set or super-set formats without the loss of information. We therefore believe that the decision as to which data format serializations are used by a given application should be guided by the choice of technology and the target audience or research field. This is consistent with the views of the committee for the Linguistic Annotation Framework (LAF) who explicitly state in the ISO CD 24612 (LAF) document (ISO (2012)); Although the LAF pivot format may be used in any context, it is assumed that users will represent annotations using their own formats, which can then be transduced to the LAF pivot format for the purposes of exchange, merging and comparison. The transduction of an EMU annotation into a format such as the LAF pivot format is a simple process, as they share many of the same concepts and are well defined formats. 4.1 Per database annotation structure definition Unlike other systems, the EMU-SDMS requires the user to define the annotation structure formally for all annotations within a database. Much as Document Type Definition (DTD) or XML Schema Definition (XSD) describe the syntactically valid elements in an Extensible Markup Language (XML) document, the database configuration file of an emuDB defines the valid annotation levels and therefore the type of items that are allowed to be present in a database. Unlike DTDs or XSDs, the configuration file can also define semantic relationships between annotation levels which fall outside the scope of traditional, syntactically oriented schema definitions and validation. This global definition of an annotation structure has numerous benefits for the data integrity of the database, as the EMU-SDMS can perform consistency checks and prevent malformed as well as semantically void annotation structures.6 Because of these formal definitions, the EMU system generally distinguishes between the actual representations of a structural element which are contained within the database and their formal definitions. An example of an actual representation, that is a subset of the actual annotation, would be a level contained in an annotation file that contains SEGMENTs that annotate a recording. The corresponding formal definition would be a level definition entry in the database’s configuration file, which specifies and validates the level’s existence within the database. As mentioned above, the actual annotation files of an emuDB contain the annotation items as well as their hierarchical linking information. To be able to check the validity of a connection between two items, the user specifies which links are permitted for the entire database just as for the level definitions. The permitted hierarchical relationships in an emuDB are expressed through link definitions between level definitions as part of the database configuration. There are three types of valid hierarchical relationships between levels: ONE\\_TO\\_MANY, MANY_TO_MANY and ONE_TO_ONE. These link definitions specify the permitted relationships between instances of annotation items of one level and those of another. The structure of the hierarchy that corresponds to the annotation depicted in Figure @ref(fig:annot_structhybridAnnot)C can be seen in Figure 4.2A. The structure in Figure 4.2A is a typical example of an EMU hierarchy where only the level of type SEGMENT contains time information and the others are timeless as they are of the type ITEM. The top three levels, Text, Syllable and Phoneme, have a ONE_TO_MANY relationship specifying that a single item in the parent level may have a dominance relationship with multiple items in the child level. In this example, the relationship between and Phonetic is MANY_TO_MANY: this type of relationship can be used to represent schwa elision and subsequent sonorant syllabification, as when the final syllable of sudden is d@n at the Phoneme level but dn at the Phonetic level. Figure 4.2B displays an example of a more complex, intersecting hierarchical structure definition where Abercrombian feet (Abercombie (1967)) are incorporated into the tones and break indices (ToBI) (Beckman and Ayers (1997)) prosodic hierarchy by allowing an intonational phrase to be made up of one or more feet (for further details see Harrington (2010) page 98). Figure 4.2: A: a schematic representation of the hierarchical structure of an emuDB that corresponds to the annotation depicted in (???)(fig:annot-structhybridAnnot)C; B: example of a more complex, intersecting hierarchical structure. Based on our experience, the explicit definition of the annotation structure for every database which was also integral to the legacy system addresses the excessively expressive nature of annotational modeling systems mentioned in Bird and Liberman (2001). Although, in theory, infinitely large hierarchies can be defined for a database, users of the legacy system typically chose to use only moderately complex annotation structures. The largest hierarchy definitions we have encountered spanned up to fifteen levels while the average amount of levels was between three and five. This self-restriction is largely due to the primary focus of speech and spoken language domain-specific annotations, as the number of annotation levels between chunks of speech above the word level (intonational phrases/sentences/turns/etc.) and the lower levels (phonetic segmentation/EMA gestural landmark annotation/tone annotation/etc.) is a finite set. 4.2 Parallel labels and multiple attributes The legacy EMU system made a distinction between linearly and non-linearly linked inter-level links. Linearly linked levels were used to describe, enrich or supplement another level. For example, a level called Category might have been included as a separate level from Word for marking words’ grammatical category memberships (thus each word might be marked as one of adjective, noun, verb, etc.), or information about whether or not a syllable is stressed might be included on a separate Stress tier (description taken from Harrington (2010) page 77). Using ONE_TO_ONE link definitions to define a relationship between two levels, it is still possible to model linearly linked levels in the new EMU-SDMS. However, an additional, cleaner concept that reduces the extra level overhead has been implemented that allows every annotation item to carry multiple attributes (i.e., labels). Further, using this construct reduces the number of levels, items and links and therefore the hierarchical complexity of an annotation. The generic term “attribute”&quot; (vs. “label”) was chosen to have the flexibility of adding attributes that are not of the type STRING (i.e., labels) to the annotation modeling capabilities of the EMU-SDMS in future versions. Figure @ref(fig:paraLabels} shows the annotation structure modeling difference between linearly linked levels (see Figure @ref(fig:paraLabels}A) and an annotation structure using multiple attributes (see Figure @ref(fig:paraLabels}B). Figure @ref(fig:paraLabels}A shows three separate levels (Word, Accent and Text) that have a ONE_TO_ONE relationship. Each of their annotation items is linked to exactly one annotation item in the child level (e.g., A1-A3). Figure @ref(fig:paraLabels}B shows a single level that has three attribute definitions (Word, Accent and Text) and each annotation item contains three attributes (e.g., A1-A3). Figure 4.3: Schematic representation of annotation structure modeling difference between A: linearly linked levels and B: an annotation structure using multiple attributes. It is worth noting that every level definition must have an attribute definition which matches its level name. This primary attribute definition must also be present in every annotation item belonging to a level. As emuR’s database interaction functions, such as add_levelDefinition(), and the EMU-webApp automatically perform the necessary actions this should only be of interest to (semi-)advanced users wishing to automatically generate the _annot.json format. 4.3 Metadata strategy using single bundle root nodes As the legacy EMU system and the new EMU-SDMS do not have an explicit method for storing metadata associated with bundles7, over the years an annotation structure convention has been developed to combat this issue. The convention is to use a generic top level (often simply called bundle) that contains a single annotation item in every annotation file. Using the multiple attribute annotation structure modeling capability of the EMU-SDMS, this single annotation item can hold any meta data associated with the bundle. Additionally linking the item to all the annotation items of its child level effectively makes it a parent to every item of the hierarchy. This linking information can later be exploited to query only bundles with matching meta data (see Chapter @ref(chap:querysys} for details). Figure @ref(fig:singleBundleRootNode} displays a hierarchical annotation where the top level (bundle) contains information about the speaker’s gender, the city of birth (COB) and age. Figure 4.4: Hierarchical annotation displaying single bundle root node metadata strategy where the label of the primary attribute definition (bundle) is empty, gender encodes the speaker’s gender, COB encodes the speakers city of birth and age encodes the speaker’s age in the form of a string. 4.4 Conclusion The annotation structure modeling capabilities of the EMU-SDMS surpass those of many other commonly used systems. They do so by not only allowing the use of levels containing time information (levels of type SEGMENT and EVENT) but also timeless levels (levels of type ITEM). Additionally, they allow users to define hierarchical annotation structures by allowing explicit links to be implemented from one level’s items to those of another. Although it is not obligatory to use them in the EMU-SDMS, we feel the usage of hierarchical annotations allow for complex rich data modeling and are often cleaner representations of the annotations at hand. References "],
["chap-emuDB.html", "5 The emuDB Format8 5.1 Database design 5.2 Creating an emuDB 5.3 Conclusion", " 5 The emuDB Format8 This chapter describes the emuDB format, which is the new database format of the EMU-SDMS, and shows how to create and interact with this format. The emuDB format is meant as a simple, general purpose way of storing speech databases that may contain complex, rich, hierarchical annotations as well as derived and complementary speech data. These different components will be described throughout this chapter, and examples will show how to generate and manipulate them. On designing the new EMU system, considerable effort went into designing an appropriate database format. We needed a format that was standardized, well structured, easy to maintain, easy to produce, easy to manipulate and portable. We decided on the JavaScript Object Notation (JSON) file format9 as our primary data source for several reasons. It is simple, standardized, widely-used and text-based as well as machine and human readable. In addition, this portable text format allows expert users to (semi-) automatically process and/or generate annotations. Other tools such as the BAS Webservices (Kisler, Schiel, and Sloetjes 2012) and SpeechRecorder (Draxler and Jänsch 2004) have already taken advantage of being able to produce such annotations. Using database back-end options such as relational or graph databases of either the SQL or NoSQL variety as the primary data source for annotations would not directly permit other tools to produce annotations because intermediary exchange file formats would have to be defined to permit this functionality with these back-ends. Our choice of the JSON format was also guided by the decision to incorporate web technologies as part of the EMU-SDMS for which the JSON format is the de facto standard (see Chapter 9). Further, as the default encoding of the JSON format is UTF-8 the EMU-SDMS fully supports the Unicode character set for any user-defined string within an emuDB (e.g. level names and labels)10. We chose to use the widely adopted Waveform Audio File Format (WAVE, or more commonly known as WAV due to its filename extension) as our primary media/audio format. Although some components of the EMU-SDMS, notably the wrassp package, can handle various other media/audio formats (see ?wrassp::AsspFileFormats for details) this is the only audio file format currently supported by every component of the EMU-SDMS. Nevertheless, the wrassp package can be utilized to convert files from one of it’s other supported file formats to the WAV format.11 Future releases of the EMU-SDMS might include the support of other media/audio formats. In contrast to other systems, including the legacy EMU system, we chose to fully standardize the on-disk structure of speech databases with which the system is capable of working. This provides a standardized and structured way of storing speech databases while providing the necessary amount of freedom and separability to accommodate multiple types of data. Further, this standardization enables fast parsing and simplification of file-based error tracking and simplifies database subset and merging operations as well as database portability. An overview of all database interaction functions is given in Section ??. 5.1 Database design An emuDB consists of a set of files and directories that adhere to a certain structure and naming convention (see Figure 5.1). The database root directory must include a single _DBconfig.json file that contains the configuration options of the database such as its level definitions, how these levels are linked in the database hierarchy and how the data is to be displayed by the graphical user interface. A detailed description of the _DBconfig.json file is given in Appendix ??. The database root directory also contains arbitrarily named session directories (except for the obligatory _ses suffix). These session directories can be used to group the recordings of a database in a logical manner. Sessions can be used, for example, to group all recordings from speaker AAA into a session called AAA_ses. Figure 5.1: Schematic emuDB file and directory structure. Each session directory can contain any number of _bndl directories (e.g., rec1_bndl rec2_bndl … rec9_bndl). All files belonging to a recording (i.e., all files describing the same timeline) are stored in the same bundle directory. This includes the actual recording (.wav) and can contain optional derived or supplementary signal files in the simple signal file format (SSFF) (Cassidy 2013) such as formants (.fms) or the fundamental frequency (.f0), both of which can be calculated using the wrassp package (see Chapter 8). Each bundle directory contains the annotation file (_annot.json) of that bundle (i.e., the annotations and the hierarchical linking information; see Appendix ?? for a detailed description of the file format). JSON schema files for all the JSON files types used have been developed to ensure the syntactic integrity of the database (see https://github.com/IPS-LMU/EMU-webApp/tree/master/dist/schemaFiles). All files within a bundle that are associated with that bundle must have the same basename as the _bndl directory prefix. For example, the signal file in bundle rec1_bndl must have the name rec1.wav to be recognized as belonging to the bundle. The optional _emuDBcache.sqlite file in the root directory (see Figure 5.1 contains the relational cache representation of the annotations of the emuDB (see Chapter ?? for further details). All files in an _bndl directory that do not follow the above naming conventions will simply be ignored by the database interaction functions of the emuR package. 5.2 Creating an emuDB The two main strategies for creating emuDBs are either to convert existing databases or file collections to the new format or to create new databases from scratch where only .wav audio files are present. Chapter 3 gave an example of how to create an emuDB from an existing TextGrid file collection and other conversion routines are covered in Section ??. In this chapter we will focus on creating an emuDB from scratch with nothing more than a set of .wav audio files present. 5.2.1 Creating an emuDB from scratch R Example ?? shows how an empty emuDB is created in the directory provided by R’s tempdir() function. As can be seen by the output of the list.files() function, create_emuDB() creates a directory containing a _DBconfig.json file only. # load package library(emuR, warn.conflicts = F) # create demo data in directory # provided by tempdir() create_emuRdemoData(dir = tempdir()) # create emuDB called &quot;fromScratch&quot; create_emuDB(name = &quot;fromScratch&quot;, targetDir = tempdir(), verbose = F) # generate path to the empty fromScratch created above dbPath = file.path(tempdir(), &quot;fromScratch_emuDB&quot;) # show content of empty fromScratch emuDB list.files(dbPath) ## [1] &quot;fromScratch_DBconfig.json&quot; 5.2.2 Loading and editing an empty database The initial step in manipulating and generally interacting with a database is to load the database into the current R session. R Example ?? shows how to load the fromScratch database and shows the empty configuration by displaying the output of the summary() function. # load database dbHandle = load_emuDB(dbPath, verbose = F) # show summary of dbHandle summary(dbHandle) ## Name: fromScratch ## UUID: 5d6284e4-2a5e-43f1-be49-8c80957c1537 ## Directory: /private/var/folders/yk/8z9tn7kx6hbcg_9n4c1sld980000gn/T/Rtmp4qeLlx/fromScratch_emuDB ## Session count: 0 ## Bundle count: 0 ## Annotation item count: 0 ## Label count: 0 ## Link count: 0 ## ## Database configuration: ## ## SSFF track definitions: ## NULL ## ## Level definitions: ## NULL ## ## Link definitions: ## NULL # show class vector of dbHandle class(dbHandle) ## [1] &quot;emuDBhandle&quot; As can be seen in R Example ??, the class of a loaded emuDB is emuDBhandle. A emuDBhandle object is used to reference a loaded emuDB in the database interaction functions of the emuR package. In this chapter we will show how to use this emuDBhandle object to perform database manipulation operations. Most of the emuDB manipulation functions follow the following function prefix naming convention: add_XXX add a new instance of XXX / set_XXX set the current instance of XXX, list_XXX list the current instances of XXX / get_XXX get the current instance of XXX, remove_XXX remove existing instances of XXX. 5.2.3 Level definitions Unlike other systems, the EMU-SDMS requires the user to formally define the annotation structure for the entire database. An essential structural element of any emuDB are its levels. A level is a more general term for what is often referred to as a tier. It is more general in the sense that people usually expect tiers to contain time information. Levels can either contain time information if they are of the type EVENT or of the type SEGMENT but are timeless if they are of the type ITEM (see Chapter ?? for further details). It is also worth noting that an emuDB distinguishes between the definition of an annotation structure element and the actual annotations. The definition of an annotation structure element such as a level definition is merely an entry in the _DBconfig.json file which specifies that this level is allowed to be present in the _annot.json files. The levels that are present in an _annot.json file, on the other hand, have to adhere to the definitions in the _DBconfig.json. As the fromScratch database (already loaded) does not contain any annotation structural element definitions, R Example ?? shows how a new level definition called Phonetic of type SEGMENT is added to the emuDB. # show no level definitions # are present list_levelDefinitions(dbHandle) ## NULL # add level defintion add_levelDefinition(dbHandle, name = &quot;Phonetic&quot;, type = &quot;SEGMENT&quot;) # show newly added level definition list_levelDefinitions(dbHandle) ## name type nrOfAttrDefs attrDefNames ## 1 Phonetic SEGMENT 1 Phonetic; R Example @ref(rexample:emuDB-addLevelDefWord} shows how a further level definition is added that will contain the orthographic word transcriptions for the words uttered in our recordings. This level will be of the type ITEM, meaning that elements contained within the level are sequentially ordered but do not contain any time information. # add level definition add_levelDefinition(dbHandle, name = &quot;Word&quot;, type = &quot;ITEM&quot;) # list newly added level definition list_levelDefinitions(dbHandle) ## name type nrOfAttrDefs attrDefNames ## 1 Phonetic SEGMENT 1 Phonetic; ## 2 Word ITEM 1 Word; The function remove_levelDefinition() can also be used to remove unwanted level definitions. However, as we wish to further use the levels Phonetic and Word, we will not make use of this function here. 5.2.3.1 Attribute definitions Each level definition can contain multiple attributes, the most common, and currently only supported attribute being a label (of type STRING). Thus it is possible to have multiple parallel labels (i.e., attribute definitions) in a single level. This means that a single annotation item instance can contain multiple labels while sharing other properties such as the start and duration information. This can be useful when modeling certain types of data. An example of this would be the Phonetic level created above. It is often the case that databases contain both the phonetic transcript using IPA UTF-8 symbols as well as a transcript using Speech Assessment Methods Phonetic Alphabet (SAMPA) symbols. To avoid redundant time information, both of these annotations can be stored on the same Phonetic level using multiple attribute definitions (i.e., parallel labels). R Example ?? shows the current attribute definitions of the Phonetic level. # list attribute definitions of &#39;Phonetic&#39; level list_attributeDefinitions(dbHandle, levelName = &quot;Phonetic&quot;) ## name level type hasLabelGroups hasLegalLabels ## 1 Phonetic Phonetic STRING FALSE FALSE Even though no attribute definition has been added to the Phonetic level, it already contains an attribute definition that has the same name as its level. This attribute definition represents the obligatory primary attribute of that level. As every level must contain an attribute definition that has the same name as its level, it is automatically added by the add_levelDefinition() function. To follow the above example, R Example ?? adds a further attribute definition to the Phonetic level that contains the SAMPA versions of our annotations. # add add_attributeDefinition(dbHandle, levelName = &quot;Phonetic&quot;, name = &quot;SAMPA&quot;) ## NULL # list attribute definitions of &#39;Phonetic&#39; level list_attributeDefinitions(dbHandle, levelName = &quot;Phonetic&quot;) ## name level type hasLabelGroups hasLegalLabels ## 1 Phonetic Phonetic STRING FALSE FALSE ## 2 SAMPA Phonetic STRING FALSE FALSE 5.2.3.2 Legal labels As can be inferred from the columns hasLabelGroups and hasLegalLabels of the output of the above list_attributeDefinitions() function, attribute definitions can also contain two further optional fields. The legalLabels field contains an array of strings that specifies the labels that are legal (i.e., allowed or valid) for the given attribute definition. As the EMU-webApp does not allow the annotator to enter any labels that are not specified in this array, this is a simple way of assuring that a level has a consistent label set. R Example ?? shows how the set_legalLabels and get_legalLabels functions can be used to specify a legal label set for the primary Word attribute definition of the Word level. # define allowed word labels wordLabels = c(&quot;amongst&quot;, &quot;any&quot;, &quot;are&quot;, &quot;always&quot;, &quot;and&quot;, &quot;attracts&quot;) # show empty legal labels # for &quot;Word&quot; attribute definition get_legalLabels(dbHandle, levelName = &quot;Word&quot;, attributeDefinitionName = &quot;Word&quot;) ## [1] NA # set legal labels values # for &quot;Word&quot; attribute definition set_legalLabels(dbHandle, levelName = &quot;Word&quot;, attributeDefinitionName = &quot;Word&quot;, legalLabels = wordLabels) # show recently added legal labels # for &quot;Word&quot; attribute definition get_legalLabels(dbHandle, levelName = &quot;Word&quot;, attributeDefinitionName = &quot;Word&quot;) ## [1] &quot;amongst&quot; &quot;any&quot; &quot;are&quot; &quot;always&quot; &quot;and&quot; &quot;attracts&quot; 5.2.3.3 Label groups A further optional field is the labelGroups field. It contains specifications of groups of labels that can be referenced by a name given to the group while querying the emuDB. R Example ?? shows how the add_attrDefLabelGroup() function is used to add two label groups to the Phonetic attribute definition. One of the groups is used to reference a subset of longVowels and the other to reference a subset of shortVowels on the Phonetic level. # add long vowels label group add_attrDefLabelGroup(dbHandle, levelName = &quot;Phonetic&quot;, attributeDefinitionName = &quot;Phonetic&quot;, labelGroupName = &quot;longVowels&quot;, labelGroupValues = c(&quot;i:&quot;, &quot;u:&quot;)) # add short vowels label group add_attrDefLabelGroup(dbHandle, levelName = &quot;Phonetic&quot;, attributeDefinitionName = &quot;Phonetic&quot;, labelGroupName = &quot;shortVowels&quot;, labelGroupValues = c(&quot;i&quot;, &quot;u&quot;, &quot;@&quot;)) # list current label groups list_attrDefLabelGroups(dbHandle, levelName = &quot;Phonetic&quot;, attributeDefinitionName = &quot;Phonetic&quot;) ## name values ## 1 longVowels i:; u: ## 2 shortVowels i; u; @ # query all short vowels # Note the result of this query # is empty as no annotations are present # in the &#39;fromScratch&#39; emuDB query(dbHandle, &quot;Phonetic == shortVowels&quot;) ## segment list from database: fromScratch ## query was: Phonetic == shortVowels ## [1] labels start end session bundle level type ## &lt;0 rows&gt; (or 0-length row.names) For users who are familiar with or transitioning from the legacy EMU system, it is worth noting that the label groups correspond to the unfavorably named Legal Labels entries of the GTemplate Editor (i.e., legal entries in the .tpl file) of the legacy system. In the new system the legalLabels entries specify the legal or allowed label values of attribute definitions while the labelGroups specify groups of labels that can be referenced by the names given to the groups while performing queries. A new feature of the EMU-SDMS is the possibility of defining label groups for the entire emuDB as opposed to a single attribute definition (see ?add_labelGroups for further details). This avoids the redundant definition of label groups that should span multiple attribute definitions (e.g., a longVowels subset that is to be queried on a level called Phonetic_1 as well as a level called Phonetic_2). 5.2.4 Link definitions An essential and very powerful conceptual and structural element of any emuDB is its hierarchy. Using hierarchical structures is highly recommended but not a must. Hierarchical annotations allow for complex, rich data modeling and are often cleaner representations of the annotations at hand. As Chapter ?? contains in-depth explanations of the annotation modeling capabilities of the EMU-SDMS and Chapter 6 shows how these structures can be queried using emuR’s query mechanics, this chapter will omit an explanation of hierarchical annotation structures. R Example ?? shows how a ONE_TO_MANY relationship between the Word and Phonetic in the form of a link definition is added to an emuDB. # show that currently no link definitions # are present list_linkDefinitions(dbHandle) ## NULL # add new &quot;ONE_TO_MANY&quot; link definition # between &quot;Word&quot; and &quot;Phonetic&quot; levels add_linkDefinition(dbHandle, type = &quot;ONE_TO_MANY&quot;, superlevelName = &quot;Word&quot;, sublevelName = &quot;Phonetic&quot;) # show newly added link definition list_linkDefinitions(dbHandle) ## type superlevelName sublevelName ## 1 ONE_TO_MANY Word Phonetic A schematic of the simple hierarchical structure of the fromScratch created by R Example ?? is displayed in Figure 5.2. Figure 5.2: A schematic representation of the simple hierarchical structure of the fromScratch created by the add_linkDefinition() function call in R Example (???)(rexample:emuDB-addLinkDef). 5.2.5 File handling The previous sections of this chapter defined the simple structure of the fromScratch emuDB. An essential element that is still missing from the emuDB is the actual audio speech data12. R Example ?? shows how the import_mediaFiles() function can be used to import audio files, referred to as media files in the context of an emuDB, into the fromScratch emuDB. # get the path to directory containing .wav files wavDir = file.path(tempdir(), &quot;emuR_demoData&quot;, &quot;txt_collection&quot;) # Import media files into emuDB session called fromWavFiles. # Note that the txt_collection directory also contains .txt files. # These are simply ignored by the import_mediaFiles() function. import_mediaFiles(dbHandle, dir = wavDir, targetSessionName = &quot;fromWavFiles&quot;, verbose = F) # list session list_sessions(dbHandle) ## name ## 1 fromWavFiles # list bundles list_bundles(dbHandle) ## session name ## 1 fromWavFiles msajc003 ## 2 fromWavFiles msajc010 ## 3 fromWavFiles msajc012 ## 4 fromWavFiles msajc015 ## 5 fromWavFiles msajc022 ## 6 fromWavFiles msajc023 ## 7 fromWavFiles msajc057 # show first two files in the emuDB library(tibble) # convert to tibble only to prettify output as_tibble(head(list_files(dbHandle), n = 2)) ## # A tibble: 2 x 4 ## session bundle file absolute_file_path ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 fromWavFiles msajc003 msajc003_annot.json /private/var/folders/yk/8z9tn… ## 2 fromWavFiles msajc003 msajc003.wav /private/var/folders/yk/8z9tn… The import_mediaFiles() call in R Example ?? added a new session called fromWavFiles to the fromScratch emuDB containing a new bundle for each of the imported media files. The annotations of every bundle, despite containing empty levels, adhere to the structure specified above. This means that every _annot.json file created contains an empty Word and Phonetic level array and the links array is also empty. The emuR package also provides a mechanism for adding files to preexisting bundle directories, as this can be quite tedious to perform manually due to the nested directory structure of an emuDB. R Example ?? shows how preexisting .zcr files that are produced by wrassp’s zcrana() function can be added to the preexisting session and bundle structure. As the directory referenced by wavDir does not contain any .zcr files, R Example ?? first creates them and then adds them to the emuDB (see Chapter 8 for further details). # load wrassp package library(wrassp) # list all wav files in wavDir wavFilePaths = list.files(wavDir, pattern = &quot;.*.wav&quot;, full.names = TRUE) # calculate zero-crossing-rate files # using zcrana function of wrassp package zcrana(listOfFiles = wavFilePaths, verbose = FALSE) ## [1] 7 # add zcr files to emuDB add_files(dbHandle, dir = wavDir, fileExtension = &quot;zcr&quot;, targetSessionName = &quot;fromWavFiles&quot;) # show first three files in emuDB (convert to tibble only # to prettify output) as_tibble(head(list_files(dbHandle), n = 3)) ## # A tibble: 3 x 4 ## session bundle file absolute_file_path ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 fromWavFiles msajc003 msajc003_annot.json /private/var/folders/yk/8z9tn… ## 2 fromWavFiles msajc003 msajc003.wav /private/var/folders/yk/8z9tn… ## 3 fromWavFiles msajc003 msajc003.zcr /private/var/folders/yk/8z9tn… 5.2.6 SSFF track definitions A further important structural element of any emuDB is use of the so-called SSFF tracks, which are often simply referred to as tracks. These SSFF tracks reference data that is stored in the SSFF (see Appendix ?? for a detailed description of the file format) within the _bndl directories. The two main types of data are: complementary data that was acquired during the recording such as by EMA or EPG; or derived data, that is data that was calculated from the original audio signal such as formant values and their bandwidths or the short-term Root Mean Square amplitude of the signal. As Section ?? covers how the SSFF file output of a wrassp function can be added to an emuDB, an explanation will be omitted here. R Example ?? shows how the .zcr files added in R Example ?? can be added as an SSFF track definition (see Chapter 8 for further details). # show that no SSFF track definitions # are present list_ssffTrackDefinitions(dbHandle) ## NULL # add SSFF track definition to emuDB add_ssffTrackDefinition(dbHandle, name = &quot;zeroCrossing&quot;, columnName = &quot;zcr&quot;, fileExtension = &quot;zcr&quot;) # show newly added SSFF track definition list_ssffTrackDefinitions(dbHandle) ## name columnName fileExtension ## 1 zeroCrossing zcr zcr 5.2.7 Configuring the EMU-webApp and annotating the emuDB As previously mentioned, the current fromScratch emuDB contains only empty levels. In order to start annotating the database, the EMU-webApp has to be configured to display the desired information. Although the configuration of the EMU-webApp is stored in the _DBconfig.json file and is therefore a part of the emuDB format, here we will omit an explanation of the extensive possibilities of configuring the web application (see Chapter 9 for an in-depth explanation). R Example ?? shows how the Phonetic level is added to the level canvases order array of the default perspective. # show empty level canvases order get_levelCanvasesOrder(dbHandle, perspectiveName = &quot;default&quot;) ## NULL # set level canvases order to display &quot;Phonetic&quot; level set_levelCanvasesOrder(dbHandle, perspectiveName = &quot;default&quot;, order = c(&quot;Phonetic&quot;)) # show newly added level canvases order get_levelCanvasesOrder(dbHandle, perspectiveName = &quot;default&quot;) ## [1] &quot;Phonetic&quot; As a final step before beginning the annotation process, the fromScratch emuDB has to be served to the EMU-webApp for annotation and visualization purposes. R Example ?? shows how this can be achieved using the serve() function. # serve &quot;fromScratch&quot; emuDB to the EMU-webApp serve(dbHandle) 5.3 Conclusion This chapter introduced the elements that comprise the new emuDB format and provided a practical overview of the essential database interaction functions provided by the emuR package. We feel the emuDB format provides a general purpose, flexible approach to storing speech databases with the added benefit of being able to directly manipulate and analyse these databases using the tools provided by the EMU-SDMS. References "],
["chap-querysys.html", "6 The query system 6.1 emuRsegs: The resulting object of a query 6.2 EQL: The EMU Query Language version 2 6.3 Discussion", " 6 The query system This chapter describes the newly implemented query system of the emuR package. When developing the new emuR package it was essential that it had a query mechanism allowing users to query a database’s annotations in a simple manner. The EMU query language (EQL) of the EMU-SDMS arose out of years of developing and improving upon the query language of the legacy system (e.g., Cassidy and Harrington (2001), Harrington (2010), John (2012)). As a result, today we have an expressive, powerful, yet simple to learn and domain-specific query language. The EQL defines a user interface by allowing the user to formulate a formal language expression in the form of a query string. The evaluation of a query string results in a set of annotation items or, alternatively, a sequence of items of a single annotation level in the emuDB from which time information, if applicable (see Section ??), has been deduced from the time-bearing sub-level. An example of this is a simple query that extracts all strong syllables (i.e., syllable annotation items containing the label S on the Syllable level) from a set of hierarchical annotations (see Figure 6.1 for an example of a hierarchical annotation). The respective EQL query string &quot;Syllable == S&quot; results in a set of segments containing the annotation label S. Due to the temporal inclusion constraint of the domination relationship, the start and end times of the queried segments are derived from the respective items of the Phonetic level (i.e., the m and H nodes in Figure 6.1, as this is the time-bearing sub-level. The EQL described here allows users to query the complex hierarchical annotation structures in their entirety as they are described in Chapter ??. Figure 6.1: Simple partial hierarchy of an annotation of the word amongst in the msajc003 bundle in the ae demo emuDB. R Example ?? shows how to create the demo data that is provided by the emuR package followed by loading an example emuDB called ae into the current R session. This database will be used in all the examples throughout this chapter. # load package library(emuR) # create demo data in directory # provided by tempdir() create_emuRdemoData(dir = tempdir()) # create path to demo database path2ae = file.path(tempdir(), &quot;emuR_demoData&quot;, &quot;ae_emuDB&quot;) # load database ae = load_emuDB(path2ae, verbose = F) 6.1 emuRsegs: The resulting object of a query In emuR the result of a query or requery (see Section 6.2.7) is a pre-specified object which is a superclass of the common data.frame. R Example ?? shows the result of a slightly expanded version of the above query (&quot;Syllable == S&quot;), which additionally uses the dominates operator (i.e., the ^ operator; for further information see Section @ref(subsubsec:query_dominationQueries)) to reduce the queried annotations to the partial hierarchy depicted in Figure @ref(fig:amongstHier} in the ae demo emuDB. In this example, the classes of the resulting object including its printed output are displayed. The class vector of a resulting emuRsegs object also contains the legacy EMU system’s emusegs class, which indicates that this object is fully backwards compatible with the legacy class and the methods available for it (see Harrington (2010) for details). The printed output provides information about which database was queried and what the query was as well as information about the labels, start and end times (in milliseconds), session, bundle, level and type information. The call to colnames() shows that the resulting object has additional columns, which are ignored by the print() function. This somewhat hidden information is used to store information about what the exact items or sequence of items were retrieved from the emuDB. This information is needed to know which items to start from in a requery (see Section @ref(subsec:requery}) and is also the reason why an emuRsegs object should be viewed as a reference of sequences of annotation items that belong to a single level in all annotation files of an emuDB. # query database sl = query(ae, &quot;[Syllable == S ^ Text == amongst]&quot;) # show class vector class(sl) ## [1] &quot;emuRsegs&quot; &quot;emusegs&quot; &quot;data.frame&quot; # show sl object sl ## segment list from database: ae ## query was: [Syllable == S ^ Text == amongst] ## labels start end session bundle level type ## 1 S 256.925 674.175 0000 msajc003 Syllable ITEM # show all (incl. hidden) column names colnames(sl) ## [1] &quot;labels&quot; &quot;start&quot; &quot;end&quot; ## [4] &quot;utts&quot; &quot;db_uuid&quot; &quot;session&quot; ## [7] &quot;bundle&quot; &quot;start_item_id&quot; &quot;end_item_id&quot; ## [10] &quot;level&quot; &quot;start_item_seq_idx&quot; &quot;end_item_seq_idx&quot; ## [13] &quot;type&quot; &quot;sample_start&quot; &quot;sample_end&quot; ## [16] &quot;sample_rate&quot; 6.2 EQL: The EMU Query Language version 2 The EQL user interface was retained from the legacy system because it was sufficiently flexible and expressive enough to meet the query needs in most types of speech science research. The EQL parser implemented in emuR is based on the Extended Backus–Naur form (EBNF) (Garshol 2003) formal language definition of John (2012), which defines the symbols and the relationship of those symbols to each other on which this language is built (see adapted version of entire EBNF in Appendix ??). Here we will describe the various terms and components that comprise the slightly adapted version 2 of the EQL. It is worth noting that the new query mechanism uses a relational back-end to handle the various query operations (see Chapter ?? for details). This means that expert users, who are proficient in Structured Query Language (SQL) may also query this relational back-end directly. However, we feel the EQL provides a simple abstraction layer which is sufficient for most speech and spoken language research. 6.2.1 Simple queries The most basic form of an EQL query is a simple equality, inequality, matching or non-matching query, two of which are displayed in R Example ??. The syntax of a simple query term is [L OPERATOR A], where L specifies a level (or alternatively the name of a parallel attribute definition); OPERATOR is one of == (equality), !$=$ (inequality), =~ (matching) or !~ (non-matching); and A is an expression specifying the labels of the annotation items of L.13 The second query in R Example ?? queries an event level. The result of querying an event level contains the same information as that of a segment level query except that the derived end times have the value zero. # query all annotation items containing # the label &quot;m&quot; on the &quot;Phonetic&quot; level sl = query(ae, &quot;Phonetic == m&quot;) # query all items NOT containing the # label &quot;H*&quot; on the &quot;Tone&quot; level sl = query(ae, &quot;Tone != H*&quot;) # show first entry of sl head(sl, n = 1) ## event list from database: ae ## query was: Tone != H* ## labels start end session bundle level type ## 1 L- 1107 0 0000 msajc003 Tone EVENT R Example ?? queries two levels that contain time information: a segment level and an event level. As described in Chapter ??, annotations in the EMU-SDMS may also contain levels that do not contain time information. R Example ?? shows a query that queries annotation items on a level that does not contain time information (the Syllable level) to show that the result contains deduced time information from the time-bearing sub-level. # query all annotation items containing # the label S on the Syllable level sl = query(ae, &quot;Syllable == S&quot;) # show first entry of sl head(sl, n = 1) ## segment list from database: ae ## query was: Syllable == S ## labels start end session bundle level type ## 1 S 256.925 674.175 0000 msajc003 Syllable ITEM 6.2.1.1 Queries using regular expressions The slightly expanded version 2 of the EQL, which comes with the emuR package, introduces regular expression operators (=~ and !~). These allow users to formulate regular expressions for more expressive and precise pattern matching of annotations. A minimal set of examples displaying the new regular expression operators is shown in Table ??. ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union Table 6.1: EQL V2: examples of simple and complex query strings using RegEx operators including their function descriptions. Query Function Phonetic =~ '[AIOUEV]' A disjunction of annotations using a RegEx character class Word =~ a.* All words beginning with a Word !~ .*st All words not ending in st [Phonetic == n ^ #Syllable =~ .*] All syllables that dominate an n segment of the Phonetic level 6.2.2 Combining simple queries The EQL contains three operators that can be used to combine the simple query terms described above as well as position queries which we will describe below. These three operators are the sequence operator, -&gt;; the conjunction operator, &amp;; and the domination operator, ^, which is used to perform hierarchical queries. These three types of queries are described below. To start with, we describe the two types of queries that query more complex annotation structures on the same level (sequence and conjunction queries). This is followed by a description of domination queries that query hierarchically linked annotation structures, sometimes spanning multiple annotation levels. 6.2.2.1 Sequence queries The syntax of a query string using the -&gt; sequence operator is [L == A -&gt; L == B] where annotation item A on level L precedes item B on level L. For a sequence query to work, both arguments must be on the same level. Alternatively parallel attribute definitions of the same level may also be chosen (see Chapter ?? for further details). An example of a query string using the sequence operator is displayed in R Example ??. All rows in the resulting segment list have the start time of @, the end time of n and their labels are @-&gt;n, where the -&gt; substring denotes the sequence. # query all sequences of items on the &quot;Phonetic&quot; level # in which an item containing the label &quot;@&quot; is followed by # an item containing the label &quot;n&quot; sl = query(ae, &quot;[Phonetic == @ -&gt; Phonetic == n]&quot;) # show first entry of sl head(sl, n = 1) ## segment list from database: ae ## query was: [Phonetic == @ -&gt; Phonetic == n] ## labels start end session bundle level type ## 1 @-&gt;n 1715.425 1791.425 0000 msajc003 Phonetic SEGMENT 6.2.2.2 Result modifier Because users are often interested in just one element of a compound query such as sequence queries (e.g., the @s in a @-&gt;n sequences), the EQL offers a so-called result modifier symbol, #. This symbol may be placed in front of any simple query component of a multi component query as depicted in R Example ??. Placing the hashtag in front of either the left or the right simple query term will result in segment lists that contains only the annotation items of the simple query term that have the hashtag in front of it. Only one result modifier may be used per query. # query the &quot;@&quot;s in &quot;@-&gt;n&quot; sequences sl = query(ae, &quot;[#Phonetic == @ -&gt; Phonetic == n]&quot;) # show first entry of sl head(sl, n = 1) ## segment list from database: ae ## query was: [#Phonetic == @ -&gt; Phonetic == n] ## labels start end session bundle level type ## 1 @ 1715.425 1741.425 0000 msajc003 Phonetic SEGMENT # query the &quot;n&quot;s in a &quot;@-&gt;n&quot; sequences sl = query(ae, &quot;[Phonetic == @ -&gt; #Phonetic == n]&quot;) # show first entry of sl head(sl, n = 1) ## segment list from database: ae ## query was: [Phonetic == @ -&gt; #Phonetic == n] ## labels start end session bundle level type ## 1 n 1741.425 1791.425 0000 msajc003 Phonetic SEGMENT 6.2.2.3 Conjunction queries The syntax of a query string using the conjunction operator can schematically be written as: [L_a1 == A &amp; L_a2 == B &amp; L_a3 == C &amp; L_a4 == D &amp; ... &amp; L_an == N], where annotation items on level L have the label A and also have the parallel labels B, C, D, …, N (see Chapter ?? for more information about parallel labels). By analogy with the sequence operator, all simple query statements must refer to the same level (i.e., only parallel attributes definitions of the same level indicated by the a1 - an may to be chosen). Hence, the conjunction operator is used to combine query conditions on the same level. Using the conjunction operator is useful for two reasons: It combines different attributes of the same level: [Text == always &amp; Accent == S] where Text and Accent are additional attributes of level Word; and It combines a simple query with a function query (see Position Queries Section ??): [Phonetic == l &amp; Start(Word, Phonetic) == 1]. An example of a query string using the conjunction operator is displayed in R Example ??. # query all words with the orthographic transcription &quot;always&quot; # that also have a strong word accent (&quot;S&quot;) query(ae, &quot;[Text == always &amp; Accent == S]&quot;) ## segment list from database: ae ## query was: [Text == always &amp; Accent == S] ## labels start end session bundle level type ## 1 always 775.475 1280.175 0000 msajc022 Text ITEM R Example ?? does not make use of the result modifier symbol. However, only the annotation items of the left simple query term (Text == always) are returned. This behavior is true for all EQL operators that combine simple query terms except for the sequence operator. As it is more explicit to use the result modifier to express the desired result, we recommend using the result modifier where possible. The more explicit variant of the above query which yields the same result is “[#Text == always &amp; Word == C]”. 6.2.2.4 Domination/hierarchical queries Compared to sequence and conjunction queries, a domination query using the operator ^ is not bound to a single level. Instead, it allows users to query annotation items that are directly or indirectly linked over one or more levels. Queries using the domination operator are often referred to as hierarchical queries as they provide the ability to query the hierarchical annotations in a vertical or inter-level manner. Figure 6.2 shows the same partial hierarchy as Figure 6.1 but highlights the annotational items that are dominated by the strong syllable (S) of the Syllable level. Such linked hierarchical sub-structures can be queried using hierarchical/domination queries. Figure 6.2: Partial hierarchy depicting all annotation items that are dominated by the strong syllable (S) of the Syllable level (inside dashed box). Items marked extcolor{three_color_c1}{green} belong to the extcolor{three_color_c1}{Phoneme} level, items marked extcolor{three_color_c2}{orange} belong to the extcolor{three_color_c2}{Phonetic} level and the extcolor{three_color_c3}{purple} dashed box indicates the set of items that are dominated by S. A schematic representation of a simple domination query string that retrieves all annotation items A of level L1 that are dominated by items B in level L2 (i.e., items that are directly or indirectly linked) is [L1 == A ^{ L2 == B]}. Although the domination relationship is directed the domination operator is not. This means that either items in L1 dominate items in L2 or items in L2 dominate items in L1. Note that link definitions that specify the validity of the domination have to be present in the emuDB configuration for this to work (see Chapter 5 for details). An example of a query string using the domination operator is displayed in R Example ??. # query all &quot;p&quot; phoneme items that belong # to / are dominated by a strong syllable (&quot;S&quot;) sl = query(ae, &quot;[Phoneme == p ^ Syllable == S]&quot;) # show first entry of sl head(sl, n = 1) ## segment list from database: ae ## query was: [Phoneme == p ^ Syllable == S] ## labels start end session bundle level type ## 1 p 558.575 639.575 0000 msajc015 Phoneme ITEM As with the conjunction query, if no result modifier is present, a dominates query returns the annotation items of the left simple query term. Hence, the more explicit variant of the above query is &quot;[#Phoneme == p ^ Syllable == S]&quot;. 6.2.3 Position queries The EQL has three function terms that specify where in a domination relationship a child level annotation item is allowed to occur. The three function terms are Start(), End() and Medial(). A schematic representation of a query string representing a simple usage of the Start(), End() and Medial() function would be: POSFCT(L1, L2) == TRUE. In this representation POSFCT is a placeholder for one of the three functions, at which level L1 must dominate level L2. Where L1 does indeed dominate L2, the corresponding item from level L2 is returned. If the expression is set to FALSE (i.e., POSFCT(L1, L2) == FALSE), all the items that do not match the condition of L2 are returned. An illustration of what is returned by each of the position functions depending on if they are set to TRUE or FALSE is depicted in Figure 6.3, while R Example ?? shows an example query using a position query term. Figure 6.3: Illustration of what is returned by the Start(), Medial() and End() functions depending if they are set to extbf{A:} extcolor{three_color_c1}{TRUE} (green) or extbf{B:} extcolor{three_color_c2}{FALSE} (orange). # query all phoneme items that occur # at the start of a syllable sl = query(ae, &quot;[Start(Syllable, Phoneme) == TRUE]&quot;) # show first entry of sl head(sl, n = 1) ## segment list from database: ae ## query was: [Start(Syllable, Phoneme) == TRUE] ## labels start end session bundle level type ## 1 V 187.425 256.925 0000 msajc003 Phoneme ITEM 6.2.4 Count queries A further query component of the EQL are so-called count queries. They allow the user to specify how many child nodes a parent annotation item is allowed to have. Figure @ref(fig:query_amongstHierCount) displays two syllables, one containing one phoneme and one phonetic annotation item, the other containing five phoneme and six phonetic items. Using EQL’s Num() function it is possible to specify which of the two syllables should be retrieved, depending on the number of phonemic or phonetic elements to which it is directly or indirectly linked. R Example @ref(rexample:query_countQuery) shows a query that queries all syllables that contain five phonemes. Figure 6.4: Partial hierarchy depicting a Syllable containing one Phoneme and Phonetic item (green) and a Syllable containing five Phoneme and six Phonetic items (orange). A schematic representation of a query string utilizing the count mechanism would be [Num(L1, L2) == N], where L1 contains N annotation items in L2. For this type of query to work L1 has to dominate L2 (i.e., be a parent level to L2). As the query matches a number (N), it is also possible to use the operators &gt; (more than), &lt; (less than) and != (not equal to). The resulting segment list contains items of L1. # retrieve all syllables that contain five phonemes query(ae, &quot;[Num(Syllable, Phoneme) == 5]&quot;) ## segment list from database: ae ## query was: [Num(Syllable, Phoneme) == 5] ## labels start end session bundle level type ## 1 S 256.925 674.175 0000 msajc003 Syllable ITEM ## 2 S 739.925 1289.425 0000 msajc003 Syllable ITEM ## 3 W 2228.475 2753.975 0000 msajc010 Syllable ITEM ## 4 S 1890.275 2469.525 0000 msajc022 Syllable ITEM ## 5 S 1964.425 2554.175 0000 msajc023 Syllable ITEM 6.2.5 More complex queries By using the correct bracketing, all of the above query components can be combined to formulate more complex queries that can be used to answer questions such as: Which occurrences of the word “his” follow three-syllable words which contain a schwa (@) in the first syllable? Such multi-part questions can usually be broken down into several sub-queries. These sub-queries can then be recombined to formulate the complex query. The steps to answering the above multi-part question are: Which occurrences of the word “his” …: [Text == his] … three-syllable words …: [Num(Text, Syllable) == 3] … contain a schwa (@) in the first syllable …: [Phoneme == @ ^ Start(Word, Syllable) == 1] All three can be combined by saying 2 dominates 3 ([2 ^ 3]) and these are followed by 1 ([2 ^ 3] -&gt; 1]) The combine query is depicted in R Example ??. This complex query demonstrates the expressive power of the query mechanism that the EMU-SDMS provides. # perform complex query # Note that the use of paste0() is optional, as # it is only used for formatting purposes query(ae, paste0(&quot;[[[Num(Text, Syllable) == 3] &quot;, &quot;^ [Phoneme == @ ^ Start(Word, Syllable) == 1]] &quot;, &quot;-&gt; #Text = his]&quot;)) ## segment list from database: ae ## query was: [[[Num(Text, Syllable) == 3] ^ [Phoneme == @ ^ Start(Word, Syllable) == 1]] -&gt; #Text = his] ## labels start end session bundle level type ## 1 his 2693.675 2780.725 0000 msajc015 Text ITEM As mastering these complex compound queries can require some practice, several simple as well as more complex examples that combine the various EQL components described above are available in Appendix ??. These examples provide practical examples to help users find queries suited to their needs. 6.2.6 Deducing time The default behavior of the legacy EMU system was to automatically deduce time information for queries of levels that do not contain time information. This was achieved by searching for the time-bearing sub-level and calculating the start and end times from the left-most and right-most annotation items which where directly or indirectly linked to the retrieved parent item. This upward purculation of time information is also the default behavior of the new EMU-SDMS. However, a new feature has been added to the query engine which allows the calculation of time to be switched off for a given query using the calcTimes parameter of the query() function. This is beneficial in two ways: for one, levels that do not have a time-bearing sub-level may be queried and secondly, the execution time of queries can be greatly improved. The performance increase becomes evident when performing queries on large data sets on one of the top levels of the hierarchy (e.g., Utterance or Intonational in the ae emuDB). When deducing time information for annotation items that contain large portions of the hierarchy, the query engine has to walk down large partial hierarchies to find the left-most and right-most items on the time-bearing sub-level. This can be a computationally expensive operation and is often unnecessary, especially during data exploration. R Example ?? shows the usage of this parameter by querying all of the items of the Intonational level and displaying the NA values for start and end times in the resulting segment list. It is worth noting that the missing time information excluded during the original query can be retrieved at a later point in time by performing a hierarchical requery (see Section 6.2.7) on the same level. # query all intonational items sl = query(ae, &quot;Intonational =~ .*&quot;, calcTimes = F) # show first entry of sl head(sl, n = 1) ## segment list from database: ae ## query was: Intonational =~ .* ## labels start end session bundle level type ## 1 L% NA NA 0000 msajc003 Intonational ITEM 6.2.7 Requery A popular feature of the legacy system was the ability to use the result of a query to perform an additional query, called a requery, starting from the resulting items of a query. The requery functionality was used to move either sequentially (horizontally) or hierarchically (vertically) through the hierarchical annotation structure. Although this feature technically does not extend the querying functionality (it is possible to formulate EQL queries that yield the same results as a query followed by \\(1:n\\) requeries), requeries benefit the user by breaking down the task of formulating long query terms into multiple, simpler queries. Compared with the legacy system, this feature is implemented in the emuR package in a more robust way, as unique item IDs are present in the result of a query, eliminating the need for searching the starting segments based on their time information. Examples of queries and their results within a hierarchical annotation based on a hierarchical and sequential requery as well as their EQL equivalents are illustrated in Figure 6.5. Figure 6.5: Three-step ( extcolor{three_color_c1}{query} -&gt; extcolor{three_color_c2}{requery_hier} -&gt; extcolor{three_color_c3}{requery_seq}) requery procedure, its single extcolor{darkgray}{query} counterpart and their color coded movements within the annotation hierarchy. R Example ?? illustrates how the same results of the sequential query [\\#Phonetic =~ .* -&gt; Phonetic == n] can be achieved using the requery_seq() function. Further, it shows how the requery_hier() function can be used to move vertically through the annotation structure by starting at the Syllable level and retrieving all the Phonetic items for the query result. ######################## # requery_seq() # query all &quot;n&quot; phonetic items sl_n = query(ae, &quot;Phonetic == n&quot;) # sequential requery (left shift result by 1 (== offset of -1)) # and hence retrieve all phonetic items directly preceeding # all &quot;n&quot; phonetic items sl_precn = requery_seq(ae, seglist = sl_n, offset = -1) # show first entry of sl_precn head(sl_precn, n = 1) ## segment list from database: ae ## query was: FROM REQUERY ## labels start end session bundle level type ## 1 E 949.925 1031.925 0000 msajc003 Phonetic SEGMENT ######################## # requery_hier() # query all strong syllables (S) sl_s = query(ae, &quot;Syllable == S&quot;) # hierarchical requery sl_phonetic = requery_hier(ae, seglist = sl_s, level = &quot;Phonetic&quot;) # show first entry of sl_phonetic head(sl_phonetic, n = 1) ## segment list from database: ae ## query was: FROM REQUERY ## labels start end session bundle level type ## 1 m-&gt;V-&gt;N-&gt;s-&gt;t-&gt;H 256.925 674.175 0000 msajc003 Phonetic SEGMENT 6.3 Discussion This chapter gave an overview of the abilities of the query system of the EMU-SDMS. We feel the EQL is an expressive, powerful, yet simple to learn and domain-specific query language that allows users to adequately query complex annotation structures. Further, the query system provided by the EMU-SDMS surpasses the querying capabilities of most commonly used systems. As the result of a query is a superclass of the common data.frame object, these results can easily be further processed using various R functions (e.g., to remove unwanted segments). Further, the results of queries can be used as input to the get_trackdata() function (see Chapter 7) which makes the query system a vital part in the default workflow described in Chapter 2. Although the query mechanism of the EMU-SDMS covers most linguistic annotation query needs (including co-occurrence and domination relationship child position queries), it has limitations due to its domain-specific nature, its simplicity and its predefined result type. Performing more general queries such as: What is the average age of the male speakers in the database who are taller than 1.8 meters? is not directly possible using the EQL. Even if the gender, height and age parameters are available as part of the database’s annotations (e.g., using the single bundle root node metadata strategy described in Chapter ??) they would be encoded as strings, which do not permit direct calculations or numerical comparisons. However, it is possible to answer these types of questions using a multi-step approach. One could, for example, extract all height items and convert the strings into numbers to filter the items containing a label that is greater than 1.8. These filtered items could then be used to perform two requeries to extract all male speakers and their age labels. These age labels could once again be converted into numbers to calculate their average. Although not as elegant as other languages, we have found that most questions that arise as part of studies working with spoken language database can be answered using such a multi-step process including some data manipulation in R, provided the necessary information is encoded in the database. Additionally, from the viewpoint of a speech scientist, we feel that the intuitiveness of an EQL expression (e.g., a query to extract the sibilant items for the question asked in the introduction: &quot;Phonetic == s|z|S|Z&quot;) exceeds that of a comparable general purpose query language (e.g. a semantically similar SQL statement: SELECT desired_columns FROM items AS i, labels AS l WHERE i.unique_bundle_item_id = l.uniq_bundle_item_id AND l.label = 's' OR l.label = 'z' OR l.label = 's' OR l.label = 'S' OR l.label = 'Z'). This difference becomes even more apparent with more complex EQL statements, which can have very long, complicated and sometimes multi-expression SQL counterparts. A problem which the EMU-SDMS does not explicitly address is the problem of cross-corpus searches. Different emuDBs may have varying annotation structures with varying semantics regarding the names or labels given to objects or annotation items in the databases. This means that it is very likely that a complex query formulated for a certain emuDB will fail when used to query other databases. If, however, the user either finds a query that works on every emuDB or adapts the query to extract the items she/he is interested in, a cross-corpus comparison is simple. As the result of a query and the corresponding data extraction routines are the same, regardless of database they where extracted from, these results are easily comparable. However, it is worth noting that the EMU-SDMS is completely indifferent to the semantics of labels and level names, which means it is the user’s responsibility to check if a comparison between databases is justifiable (e.g., are all segments containing the label “@” of the level “Phonetic”&quot; in all emuDBs annotating the same type of phoneme?). References "],
["chap-sigDataExtr.html", "7 Signal data extraction14 7.1 Extracting pre-defined tracks 7.2 Adding new tracks 7.3 Calculating tracks on-the-fly 7.4 The resulting object: trackdata vs. emuRtrackdata 7.5 Conclusion", " 7 Signal data extraction14 As mentioned in the default workflow of Chapter @ref(chap:overview}, after querying the symbolic annotation structure and dereferencing its time information, the result is a set of items with associated time stamps. It was necessary that the emuR package contain a mechanism for extracting signal data corresponding to this set of items. As illustrated in Chapter @ref(chap:wrassp}, wrassp provides the R ecosystem with signal data file handling capabilities as well as numerous signal processing routines. emuR can use this functionality to either obtain pre-stored signal data or calculate derived signal data that correspond to the result of a query. Figure @ref(fig:sigDataExtr}A shows a snippet of speech with overlaid annotations where the resulting SEGMENT of an example query (e.g., &quot;Phonetic == ai&quot;) is highlighted in yellow. Figure 7.1B displays a time parallel derived signal data contour as would be returned by one of wrassp’s file handling or signal processing routines. The yellow segment in Figure 7.1B marks the corresponding samples that belong to the ai segment of Figure 7.1A. Figure 7.1: Segment of speech with overlaid annotations and time parallel derived signal data contour. R Example ?? shows how to create the demo data that will be used throughout this chapter. # load the package library(emuR) # create demo data in directory provided by the tempdir() function create_emuRdemoData(dir = tempdir()) # get the path to a emuDB called &quot;ae&quot; that is part of the demo data path2directory = file.path(tempdir(), &quot;emuR_demoData&quot;, &quot;ae_emuDB&quot;) # load emuDB into current R session ae = load_emuDB(path2directory) 7.1 Extracting pre-defined tracks To access data that are stored in files, the user has to define tracks for a database that point to sequences of samples in files that match a user-specified file extension. The user-defined name of such a track can then be used to reference the track in the signal data extraction process. Internally, emuR uses wrassp to read the appropriate files from disk, extract the sample sequences that match the result of a query and return values to the user for further inspection and evaluation. R Example ?? shows how a signal track that is already defined in the ae demo database can be extracted for all annotation items on the Phonetic level containing the label ai. # list currently available tracks list_ssffTrackDefinitions(ae) ## name columnName fileExtension ## 1 dft dft dft ## 2 fm fm fms # query all &quot;ai&quot; phonetic segments ai_segs = query(ae, &quot;Phonetic == ai&quot;) # get &quot;fm&quot; track data for these segments # Note that verbose is set to FALSE # only to avoid a progress bar # being printed in this document. ai_td_fm = get_trackdata(ae, seglist = ai_segs, ssffTrackName = &quot;fm&quot;, verbose = FALSE) # show summary of ai_td_fm summary(ai_td_fm) ## Emu track data from 6 segments ## ## Data is 4 dimensional from track fm ## Mean data length is 30.5 samples Being able to access data that is stored in files is important for two main reasons. Firstly, it is possible to generate files using external programs such as VoiceSauce (Shue et al. 2011), which can export its calculated output to the general purpose SSFF file format. This file mechanism is also used to access data produced by EMA, EPG or many other forms of signal data recordings. Secondly, it is possible to track, save and access manipulated data such as formant values that have been manually corrected. It is also worth noting that the get_trackdata() function has a predefined track which is always available without it having to be defined. The name of this track is MEDIAFILE_SAMPLES which references the actual samples of the audio files of the database. R Example ?? shows how this predefined track can be used to access the audio samples belonging to the segments in ai_segs. # get media file samples ai_td_mfs = get_trackdata(ae, seglist = ai_segs, ssffTrackName = &quot;MEDIAFILE_SAMPLES&quot;, verbose = FALSE) # show summary of ai_td_fm summary(ai_td_mfs) ## Emu track data from 6 segments ## ## Data is 1 dimensional from track MEDIAFILE_SAMPLES ## Mean data length is 3064.333 samples 7.2 Adding new tracks As described in detail in Section ??, the signal processing routines provided by the wrassp package can be used to produce SSFF files containing various derived signal data (e.g., formants, fundamental frequency, etc.). R Example ?? shows how the add_ssffTrackDefinition() can be used to add a new track to the ae emuDB. Using the onTheFlyFunctionName parameter, the add_ssffTrackDefinition() function automatically executes the wrassp signal processing function ksvF0 (onTheFlyFunctionName = &quot;ksvF0&quot;) and stores the results in SSFF files in the bundle directories. # add new track and calculate # .f0 files on-the-fly using wrassp::ksvF0() add_ssffTrackDefinition(ae, name = &quot;F0&quot;, onTheFlyFunctionName = &quot;ksvF0&quot;, verbose = FALSE) # show newly added track list_ssffTrackDefinitions(ae) ## name columnName fileExtension ## 1 dft dft dft ## 2 fm fm fms ## 3 F0 F0 f0 # show newly added files library(tibble) # convert to tibble only to prettify output as_tibble(list_files(ae, fileExtension = &quot;f0&quot;)) ## # A tibble: 7 x 4 ## session bundle file absolute_file_path ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0000 msajc003 msajc003.f0 /private/var/folders/yk/8z9tn7kx6hbcg_9n4c… ## 2 0000 msajc010 msajc010.f0 /private/var/folders/yk/8z9tn7kx6hbcg_9n4c… ## 3 0000 msajc012 msajc012.f0 /private/var/folders/yk/8z9tn7kx6hbcg_9n4c… ## 4 0000 msajc015 msajc015.f0 /private/var/folders/yk/8z9tn7kx6hbcg_9n4c… ## 5 0000 msajc022 msajc022.f0 /private/var/folders/yk/8z9tn7kx6hbcg_9n4c… ## 6 0000 msajc023 msajc023.f0 /private/var/folders/yk/8z9tn7kx6hbcg_9n4c… ## 7 0000 msajc057 msajc057.f0 /private/var/folders/yk/8z9tn7kx6hbcg_9n4c… # extract newly added trackdata ai_td = get_trackdata(ae, seglist = ai_segs, ssffTrackName = &quot;F0&quot;, verbose = FALSE) # show summary of ai_td summary(ai_td) ## Emu track data from 6 segments ## ## Data is 1 dimensional from track F0 ## Mean data length is 30.5 samples 7.3 Calculating tracks on-the-fly With the wrassp package, we were able to implement a new form of signal data extraction which was not available in the legacy system. The user is now able to select one of the signal processing routines provided by wrassp and pass it on to the signal data extraction function. The signal data extraction function can then apply this wrassp function to each audio file as part of the signal data extraction process. This means that the user can quickly manipulate function parameters and evaluate the result without having to store to disk the files that would usually be generated by the various parameter experiments. In many cases this new functionality eliminates the need for defining a track definition for the entire database for temporary data analysis purposes. R Example ?? shows how the onTheFlyFunctionName parameter of the get_trackdata() function is used. ai_td_pit = get_trackdata(ae, seglist = ai_segs, onTheFlyFunctionName = &quot;mhsF0&quot;, verbose = FALSE) # show summary of ai_td summary(ai_td_pit) ## Emu track data from 6 segments ## ## Data is 1 dimensional from track pitch ## Mean data length is 30.5 samples 7.4 The resulting object: trackdata vs. emuRtrackdata The default resulting object of a call to get_trackdata() is of class trackdata (see R Example ??). The emuR package provides multiple routines such as dcut(), trapply() and dplot() for processing and visually inspecting objects of this type (see harrington:2010a and Section 3.5 for examples of how these can be used). # show class vector of ai_td_pit class(ai_td_pit) ## [1] &quot;trackdata&quot; As the trackdata object is a fairly complex nested matrix object with internal reference matrices, which can be cumbersome to work with, the emuR package introduces a new equivalent object type called emuRtrackdata that essentially is a flat data.frame object. This object type can be retrieved by setting the resultType parameter of the get_trackdata() function to emuRtrackdata. R Example ?? shows how this can be achieved. ai_emuRtd_pit = get_trackdata(ae, seglist = ai_segs, onTheFlyFunctionName = &quot;mhsF0&quot;, resultType = &quot;emuRtrackdata&quot;, verbose = FALSE) # show first row (convert to tibble only to prettify output) as_tibble(ai_emuRtd_pit[1, ]) ## # A tibble: 1 x 21 ## sl_rowIdx labels start end utts db_uuid session bundle start_item_id ## * &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 ai 863. 1016. 0000:… 0fc618… 0000 msajc… 161 ## # ... with 12 more variables: end_item_id &lt;int&gt;, level &lt;chr&gt;, ## # start_item_seq_idx &lt;int&gt;, end_item_seq_idx &lt;int&gt;, type &lt;chr&gt;, ## # sample_start &lt;int&gt;, sample_end &lt;int&gt;, sample_rate &lt;int&gt;, ## # times_orig &lt;dbl&gt;, times_rel &lt;dbl&gt;, times_norm &lt;dbl&gt;, T1 &lt;dbl&gt; # show relative time values of the first segment # (relative time values always start at 0 for every segment) ai_emuRtd_pit[ai_emuRtd_pit$sl_rowIdx == 1, ]$times_rel ## [1] 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 ## [18] 85 90 95 100 105 110 115 120 125 130 135 140 145 # show original time values of the first segment # (absolute time values always start at the original # time stamp for that sample within the track) ai_emuRtd_pit[ai_emuRtd_pit$sl_rowIdx == 1, ]$times_orig ## [1] 867.5 872.5 877.5 882.5 887.5 892.5 897.5 902.5 907.5 912.5 ## [11] 917.5 922.5 927.5 932.5 937.5 942.5 947.5 952.5 957.5 962.5 ## [21] 967.5 972.5 977.5 982.5 987.5 992.5 997.5 1002.5 1007.5 1012.5 As can be seen by the first row output of R Example ??, the emuRtrackdata object is an amalgamation of both a segment list and a trackdata object. The first sl_rowIdx column of the ai_emuRtd_pit object indicates the row index of the segment list the current row belongs to, the times_rel and times_orig columns represent the relative time and the original time of the samples contained in the current row (see R Example ??) and T1 (to Tn in n dimensional trackdata) contains the actual signal sample values. It is also worth noting that the emuR package provides a function called create_emuRtrackdata(), which allows users to create emuRtrackdata from a segment list and a trackdata object. This is beneficial as it allows trackdata objects to be processed using functions provided by the emuR package (e.g., dcut() and trapply()) and then converts them into a standardized data.frame object for further processing (e.g., using R packages such as lme4 or ggplot2 which were implemented to use with data.frame objects). R Example ?? shows how the create_emuRtrackdata() function is used. # create emuRtrackdata object ai_emuRtd_pit = create_emuRtrackdata(sl = ai_segs, td = ai_td_pit) # show first row and # selected columns of ai_emuRtd_pit ai_emuRtd_pit[1, ] ## sl_rowIdx labels start end session bundle level type ## 1 1 ai 862.875 1015.825 0000 msajc010 Phonetic SEGMENT ## times_orig times_rel times_norm T1 ## 1 867.5 0 0 134.7854 The general question remains as to when to use the trackdata and when to use the emuRtrackdata object and what the benefit of each class is. The trackdata object has a number of associated class functions (e.g. trapply(), dcut(), dcut() and eplot()) that ease data manipulation and visualization. Further, it avoids data redundancy and therefore has a smaller memory footprint than the emuRtrackdata object (this is usually negligible on current systems); however, this makes it rather difficult to work with. The emuRtrackdata object is intended as a long term replacement for the trackdata object as it contains all of the information of the corresponding trackdata object as well as its associated segment list. As is often the case with tabular data, the emuRtrackdata object carries certain redundant information (e.g. segment start and end times). However, the benefit of having a data.frame object that contains all the information needed to process the data is the ability to replace package specific functions (e.g. trapply() etc.) with standardized data.frame processing and visualization procedures that can be applied to any data.frame object independent of the package that generated it. Therefore, the knowledge that is necessary to process an emuRtrackdata object can be transferred to/from other packages which is not the case for trackdata object. Future releases of the emuR package as well as this manual will contain various examples of how to replace the functionality of the package-specific functions mentioned above with equivalent data manipulation and visualization using the dplyr as well as the ggplot2 packages. 7.5 Conclusion This chapter introduced the signal data extraction mechanics of the emuR package. The combination of the get_trackdata() function and the file handling and signal processing abilities of the wrassp package (see Chapter 8 for further details) provide the user with a flexible system for extracting derived or complementary signal data belonging to their queried annotation items. References "],
["chap-wrassp.html", "8 The R package wrassp15 8.1 Introduction 8.2 File I/0 and the AsspDataObj 8.3 Signal processing 8.4 The wrasspOutputInfos object 8.5 Formants and their bandwidths 8.6 Logging wrassp’s function calls 8.7 Using wrassp in the EMU-SDMS 8.8 Storing data in the SSFF file format 8.9 Conclusion", " 8 The R package wrassp15 8.1 Introduction This chapter gives an overview and introduction to the wrassp package. The wrassp package is a wrapper for R around Michel Scheffers’ libassp (Advanced Speech Signal Processor). The libassp library and therefore the wrassp package provide functionality for handling speech signal files in most common audio formats and for performing signal analyses common in the phonetic and speech sciences. As such, wrassp fills a gap in the R package landscape as, to our knowledge, no previous packages provided this specialized functionality. The currently available signal processing functions provided by wrassp are: acfana(): Analysis of short-term autocorrelation function afdiff(): Computes the first difference of the signal affilter(): Filters the audio signal (e.g., low-pass and high-pass) cepstrum(): Short-term cepstral analysis cssSpectrum(): Cepstral smoothed version of dftSpectrum() dftSpectrum(): Short-term DFT spectral analysis forest(): Formant estimation ksvF0(): F0 analysis of the signal lpsSpectrum(): Linear predictive smoothed version of dftSpectrum() mhsF0(): Pitch analysis of the speech signal using Michel Scheffers’ Modified Harmonic Sieve algorithm rfcana(): Linear prediction analysis rmsana(): Analysis of short-term Root Mean Square amplitude zcrana(): Analysis of the averages of the short-term positive and negative zero-crossing rates The available file handling functions are: read.AsspDataObj(): read a SSFF or audio file into an AsspDataObj, which is the in-memory equivalent of the SSFF or audio file. write.AsspDataObj(): write an AsspDataObj to file (usually SSFF or audio file formats). See R’s help() function for a comprehensive list of every function and object provided by the wrassp package is required (see R Example ??). help(package=&quot;wrassp&quot;) As the wrassp package can be used independently of the EMU-SDMS this chapter largely focuses on using it as an independent component. However, Section ?? provides an overview of how the package is integrated into the EMU-SDMS. Further, although the wrassp package has its own set of example audio files (which can be accessed in the directory provided by system.file('extdata', package='wrassp')), this chapter will use the audio and SSFF files that are part of the ae emuDB of the demo data provided by the emuR package. This is done primarily to provide an overview of what it is like using wrassp to work on files in an emuDB. R Example ?? shows how to generate this demo data followed by a listing of the files contained in a directory of a single bundle called msajc003 (see Chapter @ref(chap:emuDB} for information about the emuDB format). The output of the call to list.files() shows four files where the .dft and .fms files are in the SSFF file format (see Appendix ?? for further details). The _annot.json file contains the annotation information, and the .wav file is one of the audio files that will be used in various signal processing examples in this chapter. # load the emuR package library(emuR) # create demo data in directory # provided by tempdir() create_emuRdemoData(dir = tempdir()) # create path to demo database path2ae = file.path(tempdir(), &quot;emuR_demoData&quot;, &quot;ae_emuDB&quot;) # create path to bundle in database path2bndl = file.path(path2ae, &quot;0000_ses&quot;, &quot;msajc003_bndl&quot;) # list files in bundle directory list.files(path2bndl) ## [1] &quot;msajc003_annot.json&quot; &quot;msajc003.dft&quot; &quot;msajc003.fms&quot; ## [4] &quot;msajc003.wav&quot; 8.2 File I/0 and the AsspDataObj One of the aims of wrassp is to provide mechanisms for handling speech-related files such as audio files and derived and complementary signal files. To have an in-memory object that can hold these file types in a uniform way the wrassp package provides the AsspDataObj data type. R Example ?? shows how the read.AsspDataObj() can be used to import a .wav audio file. # load the wrassp package library(wrassp) # create path to wav file path2wav = file.path(path2bndl, &quot;msajc003.wav&quot;) # read audio file au = read.AsspDataObj(path2wav) # show class class(au) ## [1] &quot;AsspDataObj&quot; # show print() output of object print(au) ## Assp Data Object of file /var/folders/yk/8z9tn7kx6hbcg_9n4c1sld980000gn/T//RtmpQiaOhV/emuR_demoData/ae_emuDB/0000_ses/msajc003_bndl/msajc003.wav. ## Format: WAVE (binary) ## 58089 records at 20000 Hz ## Duration: 2.904450 s ## Number of tracks: 1 ## audio (1 fields) As can be seen in R Example ??, the resulting au object is of the class AsspDataObj. The output of print provides additional information about the object, such as its sampling rate, duration, data type and data structure information. Since the file we loaded is audio only, the object contains exactly one track. Further, since it is a mono file, this track only has a single field. We will later encounter different types of data with more than one track and multiple fields per track. R Example ?? shows function calls that extract the various attributes from the object (e.g., duration, sampling rate and the number of records). # show duration dur.AsspDataObj(au) ## [1] 2.90445 # show sampling rate rate.AsspDataObj(au) ## [1] 20000 # show number of records/samples numRecs.AsspDataObj(au) ## [1] 58089 # shorten filePath attribute # to 10 chars only to prettify output attr(au, &quot;filePath&quot;) = paste0(substr(attr(au, &quot;filePath&quot;), start = 1, stop = 45), &quot;...&quot;) # show additional attributes attributes(au) ## $names ## [1] &quot;audio&quot; ## ## $trackFormats ## [1] &quot;INT16&quot; ## ## $sampleRate ## [1] 20000 ## ## $filePath ## [1] &quot;/var/folders/yk/8z9tn7kx6hbcg_9n4c1sld980000g...&quot; ## ## $origFreq ## [1] 0 ## ## $startTime ## [1] 0 ## ## $startRecord ## [1] 1 ## ## $endRecord ## [1] 58089 ## ## $class ## [1] &quot;AsspDataObj&quot; ## ## $fileInfo ## [1] 21 2 The sample values belonging to a trackdata objects tracks are also stored within an AsspDataObj object. As mentioned above, the currently loaded object contains a single mono audio track. Accessing the data belonging to this track, in the form of a matrix, can be achieved using the track’s name in combination with the $ notation known from R’s common named list object. Each matrix has the same number of rows as the track has records and as many columns as the track has fields. R Example ?? shows how the audio track can be accessed. # show track names tracks.AsspDataObj(au) ## [1] &quot;audio&quot; # or an alternative way to show track names names(au) ## [1] &quot;audio&quot; # show dimensions of audio attribute dim(au$audio) ## [1] 58089 1 # show first sample value of audio attribute head(au$audio, n = 1) ## [,1] ## [1,] 64 This data can, for example, be used to generate an oscillogram of the audio file as shown in R Example ??, which produces Figure 8.1. # calculate sample time of every 10th sample samplesIdx = seq(0, numRecs.AsspDataObj(au) - 1, 10) samplesTime = samplesIdx / rate.AsspDataObj(au) # extract every 10th sample using window() function samples = window(au$audio, deltat=10) # plot samples stored in audio attribute # (only plot every 10th sample to accelerate plotting) plot(samplesTime, samples, type = &quot;l&quot;, xlab = &quot;time (s)&quot;, ylab = &quot;Audio samples (INT16)&quot;) Figure 8.1: Oscillogram generated from samples stored in the audio track of the object au. The export counterpart to read.AsspDataObj() function is write.AsspDataObj(). It is used to store in-memory AsspDataObj objects to disk and is particularly useful for converting other formats to or storing data in the SSFF file format as described in Section ??. To show how this function can be used to write a slightly altered version of the au object to a file, R Example ?? initially multiplies all the sample values of au$audio by a factor of 0.5. The resulting AsspDataObj is then written to an audio file in a temporary directory provided by R’s tempdir() function. # manipulate the audio samples au$audio = au$audio * 0.5 # write to file in directory # provided by tempdir() write.AsspDataObj(au, file.path(tempdir(), &#39;newau.wav&#39;)) 8.3 Signal processing As mentioned in the introduction to this chapter, the wrassp package is capable of more than just the mere importing and exporting of specific signal file formats. This section will focus on demonstrating three of wrassp’s signal processing functions that calculate formant values, their corresponding bandwidths, the fundamental frequency contour and the RMS energy contour. Section 8.5 and ?? demonstrates signal processing to the audio file saved under path2wav, while Section 8.5.2 adresses processing all the audio files belonging to the ae emuDB. 8.4 The wrasspOutputInfos object The wrassp package comes with the wrasspOutputInfos object, which provides information about the various signal processing functions provided by the package. The wrasspOutputInfos object stores meta information associated with the different signal processing functions wrassp provides. R Example ?? shows the names of the wrasspOutputInfos object which correspond to the function names listed in the introduction of this chapter. # show all function names names(wrasspOutputInfos) ## [1] &quot;acfana&quot; &quot;afdiff&quot; &quot;affilter&quot; &quot;cepstrum&quot; &quot;cssSpectrum&quot; ## [6] &quot;dftSpectrum&quot; &quot;ksvF0&quot; &quot;mhsF0&quot; &quot;forest&quot; &quot;lpsSpectrum&quot; ## [11] &quot;rfcana&quot; &quot;rmsana&quot; &quot;zcrana&quot; This object can be useful to get additional information about a specific wrassp function. It contains information about the default file extension ($ext), the tracks produced ($tracks) and the output file type ($outputType). R Example ?? shows this information for the forest() function. # show output info of forest function wrasspOutputInfos$forest ## $ext ## [1] &quot;fms&quot; ## ## $tracks ## [1] &quot;fm&quot; &quot;bw&quot; ## ## $outputType ## [1] &quot;SSFF&quot; The examples that follow will make use of this wrasspOutputInfos object mainly to acquire the default file extensions given by a specific wrassp signal processing function. 8.5 Formants and their bandwidths The already mentioned forest() is wrassp’s formant estimation function. The default behavior of this formant tracker is to calculate the first four formants and their bandwidths. R Example ?? shows the usage of this function. As the default behavior of every signal processing function provided by wrassp is to store its result to a file, the toFile parameter of forest() is set to FALSE to prevent this behavior. This results in the same AsspDataObj object as when exporting the result to file and then importing the file into R using read.AsspDataObj(), but circumvents the disk reading/writing overhead. # calculate formants and corresponding bandwidth values fmBwVals = forest(path2wav, toFile=F) # show class vector class(fmBwVals) ## [1] &quot;AsspDataObj&quot; # show track names tracks.AsspDataObj(fmBwVals) ## [1] &quot;fm&quot; &quot;bw&quot; # show dimensions of &quot;fm&quot; track dim(fmBwVals$fm) ## [1] 581 4 # check dimensions of tracks are the same all(dim(fmBwVals$fm) == dim(fmBwVals$bw)) ## [1] TRUE As can be seen in R Example ??, the object resulting from the forest() function is an object of class AsspDataObj with the tracks &quot;fm&quot; (formants) and &quot;bw&quot; (formant bandwidths), where both track matrices have four columns (corresponding to F1, F2, F3 and F4 in the &quot;fm&quot; track and F1bandwidth, F2bandwidth, F3bandwidth and F4bandwidth in the &quot;bw&quot; track) and 581 rows. To visualize the calculated formant values, R Example ?? shows how R’s matplot() function can be used to produce Figure 8.2. # plot the formant values matplot(seq(0, numRecs.AsspDataObj(fmBwVals) - 1) / rate.AsspDataObj(fmBwVals) + attr(fmBwVals, &quot;startTime&quot;), fmBwVals$fm, type = &quot;l&quot;, xlab = &quot;time (s)&quot;, ylab = &quot;Formant frequency (Hz)&quot;) # add legend startFormant = 1 endFormant = 4 legend(&quot;topright&quot;, legend = paste0(&quot;F&quot;, startFormant:endFormant), col = startFormant:endFormant, lty = startFormant:endFormant, bg = &quot;white&quot;) Figure 8.2: Matrix plot of formant values stored in the fm track of fmBwVals object. 8.5.1 Fundamental frequency contour The wrassp package includes two fundamental frequency estimation functions called ksvF0() and mhsF0(). R Example ?? shows the usage of the ksvF0() function, this time not utilizing the toFile parameter but rather to show an alternative procedure, reading the resulting SSFF file produced by it. It is worth noting that every signal processing function provided by wrassp creates a result file in the same directory as the audio file it was processing (except if the outputDirectory parameter is set otherwise). The default extension given by the ksvF0() is stored in wrasspOutputInfos\\$ksvF0\\$ext, which is used in R Example ?? to create the newly generated file’s path. # calculate the fundamental frequency contour ksvF0(path2wav) # create path to newly generated file path2f0file = file.path(path2bndl, paste0(&quot;msajc003.&quot;, wrasspOutputInfos$ksvF0$ext)) # read file from disk f0vals = read.AsspDataObj(path2f0file) By analogy with to the formant estimation example, R Example ?? shows how the plot() function can be used to visualize this data as in Figure 8.3. # plot the fundamental frequency contour plot(seq(0,numRecs.AsspDataObj(f0vals) - 1) / rate.AsspDataObj(f0vals) + attr(f0vals, &quot;startTime&quot;), f0vals$F0, type = &quot;l&quot;, xlab = &quot;time (s)&quot;, ylab = &quot;F0 frequency (Hz)&quot;) Figure 8.3: Plot of fundamental frequency values stored in the F0 track of f0vals object. 8.5.2 RMS energy contour The wrassp function for calculating the short-term root mean square (RMS) amplitude of the signal is called rmsana(). As its usage is analogous to the above examples, here we will focus on using it to calculate the RMS values for all the audio files of the ae emuDB. R Example ?? initially uses the list.files() function to aquire the file paths for every .wav file in the ae emuDB. As every signal processing function accepts one or multiple file paths, these file paths can simply be passed in as the main argument to the rmsana() function. As all of wrassp’s signal processing functions place their generated files in the same directory as the audio file they process, the rmsana() function will automatically place every .rms into the correct bundle directory. # list all .wav files in the ae emuDB paths2wavFiles = list.files(path2ae, pattern = &quot;.*wav$&quot;, recursive = TRUE, full.names = TRUE) # calculate the RMS energy values for all .wav files rmsana(paths2wavFiles) # list new .rms files using # wrasspOutputInfos-&gt;rmsana-&gt;ext rmsFPs = list.files(path2ae, pattern = paste0(&quot;.*&quot;, wrasspOutputInfos$rmsana$ext), recursive = TRUE, full.names = TRUE) # read first RMS file rmsvals = read.AsspDataObj(rmsFPs[1]) R Example ?? shows how the plot() function can be used to visualize this data as in Figure 8.4. # plot the RMS energy contour plot(seq(0, numRecs.AsspDataObj(rmsvals) - 1) / rate.AsspDataObj(rmsvals) + attr(rmsvals, &quot;startTime&quot;), rmsvals$rms, type = &quot;l&quot;, xlab = &quot;time (s)&quot;, ylab = &quot;RMS energy (dB)&quot;) Figure 8.4: Plot of RMS values stored in rms track of the rmsvals object. 8.6 Logging wrassp’s function calls As it can be extremely important to keep track of information about how certain files are created and calculated, every signal processing function provided by the wrassp package comes with the ability to log its function calls to a specified log file. R Example @ref(rexample:wrassp-logging} shows a call to the ksvF0() function where a single parameter was changed from its default value (windowShift = 10). The content of the created log files (shown by the call to readLines()) contains the function name, time stamp, parameters that were altered and processed file path information. It is worth noting that a log file can be reused for multiple function calls as the log function does not overwrite an existing file but merely appends new log information to it. # create path to log file in root dir of ae emuDB path2logFile = file.path(path2ae, &quot;wrassp.log&quot;) # calculate the fundamental frequency contour ksvF0(path2wav, windowShift = 10, forceToLog = T, optLogFilePath = path2logFile) ## [1] 1 # display content of log file (first 8 lines) readLines(path2logFile)[1:8] ## [1] &quot;&quot; ## [2] &quot;##################################&quot; ## [3] &quot;##################################&quot; ## [4] &quot;######## ksvF0 performed ########&quot; ## [5] &quot;Timestamp: 2018-05-09 18:44:41 &quot; ## [6] &quot;windowShift : 10 &quot; ## [7] &quot;forceToLog : T &quot; ## [8] &quot; =&gt; on files:&quot; 8.7 Using wrassp in the EMU-SDMS As shown in Section 8.5.2, the wrassp signal processing functions can be used to calculate SSFF files and place them into the appropriate bundle directories. The only thing that has to be done to make an emuDB aware of these files is to add an SSFF track definition to the emuDB as shown in R Example ??. Once added, this SSFF track can be referenced via the ssffTrackName parameter of the get_trackdata() function as shown in various examples throughout this documentation. It is worth noting that this strategy is not necessarily relevant for applying the same signal processing to an entire emuDB, as this can be achieved using the on-the-fly add_ssffTrackDefinition() method described in R Example ??. However, it becomes necessary if certain bundles are to be processed using deviating function parameters. This can, for example, be relevant when setting the minimum and maximum frequencies that are to be considered while estimating the fundamental frequencies (e.g., the maxF and minF of ksvfF0()) for female versus male speakers. # load emuDB ae = load_emuDB(path2ae) # add SSFF track defintion # that references the .rms files # calculated above # (i.e. no new files are calculated and added to the emuDB) ext = wrasspOutputInfos$rmsana$ext colName = wrasspOutputInfos$rmsana$tracks[1] add_ssffTrackDefinition(ae, name = &quot;rms&quot;, fileExtension = ext, columnName = colName) A further way to utilize wrassp’s signal processing functions as part of the EMU-SDMS is via the onTheFlyFunctionName and onTheFlyParams parameters of the add_ssffTrackDefinition() and get_trackdata() functions. Using the onTheFlyFunctionName parameter in the add_ssffTrackDefinition() function automatically calculates the SSFF files while also adding the SSFF track definition. Using this parameter with the get_trackdata() function calls the given wrassp function with the toFile parameter set to FALSE and extracts the matching segments and places them in the resulting trackdata or emuRtrackdata object. In many cases, this avoids the necessity of having SSFF track definitions in the emuDB. In both functions, the optional onTheFlyParams parameter can be used to specify the parameters that are passed into the signal processing function. R Example ?? shows how R’s formals() function can be used to get all the parameters of wrassp’s short-term positive and negative zero-crossing rate (ZCR) analysis function zrcana(). It then changes the default window size parameter to a new value and passes the parameters object into the add_ssffTrackDefinition() and get_trackdata() functions. # get all parameters of zcrana zcranaParams = formals(&quot;zcrana&quot;) # show names of parameters names(zcranaParams) ## [1] &quot;listOfFiles&quot; &quot;optLogFilePath&quot; &quot;beginTime&quot; ## [4] &quot;centerTime&quot; &quot;endTime&quot; &quot;windowShift&quot; ## [7] &quot;windowSize&quot; &quot;toFile&quot; &quot;explicitExt&quot; ## [10] &quot;outputDirectory&quot; &quot;forceToLog&quot; &quot;verbose&quot; # change window size from the default # value of 25 ms to 50 ms zcranaParams$windowSize = 50 # to have a segment list to work with # query all Phonetic &#39;n&#39; segments sl = query(ae, &quot;Phonetic == n&quot;) # get trackdata calculating ZCR values on-the-fly # using the above parameters. Note that no files # are generated. td = get_trackdata(ae, sl, onTheFlyFunctionName = &quot;zcrana&quot;, onTheFlyParams = zcranaParams, verbose = FALSE) # add SSFF track definition. Note that # this time files are generated. add_ssffTrackDefinition(ae, name = &quot;zcr&quot;, onTheFlyFunctionName = &quot;zcrana&quot;, onTheFlyParams = zcranaParams, verbose = FALSE) 8.8 Storing data in the SSFF file format One of the benefits gained by having the AsspDataObj in-memory object is that these objects can be constructed from scratch in R, as they are basically simple list objects. This means, for example, that any set of n-dimensional samples over time can be placed in a AsspDataObj and then stored as an SSFF file using the write.AsspDataObj() function. To show how this can be done, R Example ?? creates an arbitrary data sample in the form of a single cycle sine wave between \\(0\\) and \\(2*pi\\) that is made up of 16000 samples and displays it in Figure 8.5. x = seq(0, 2 * pi, length.out = 16000) sineWave = sin(x) plot(x, sineWave, type = &#39;l&#39;, xlab = &quot;x from 0 to 2*pi&quot;, ylab = &quot;&quot;) Figure 8.5: A single cycle sine wave consisting of 16000 samples. Assuming a sample rate of 16 kHz sineWave would result in a sine wave with a frequency of 1 Hz and a duration of one second. R Example ?? shows how a AsspDataObj can be created from scratch and the data in sineWave placed into one of its tracks. It then goes on to write the AsspDataObj object to an SSFF file. # create empty list object ado = list() # add sample rate attribute attr(ado, &quot;sampleRate&quot;) = 16000 # add start time attribute attr(ado, &quot;startTime&quot;) = 0 # add start record attribute attr(ado, &quot;startRecord&quot;) = as.integer(1) # add end record attribute attr(ado, &quot;endRecord&quot;) = as.integer(length(sineWave)) # set class of ado class(ado) = &quot;AsspDataObj&quot; # show available file formats AsspFileFormats ## RAW ASP_A ASP_B XASSP IPDS_M IPDS_S AIFF AIFC CSL ## 1 2 3 4 5 6 7 8 9 ## CSRE ESPS ILS KTH SWELL SNACK SFS SND AU ## 10 11 12 13 13 13 14 15 15 ## NIST SPHERE PRAAT_S PRAAT_L PRAAT_B SSFF WAVE WAVE_X XLABEL ## 16 16 17 18 19 20 21 22 24 ## YORK UWM ## 25 26 # set file format to SSFF # NOTE: assignment of &quot;SSFF&quot; also possible AsspFileFormat(ado) = as.integer(20) # set data format (1 == &#39;ascii&#39; and 2 == &#39;binary&#39;) AsspDataFormat(ado) = as.integer(2) # set track format specifiers # (available track formats for numbers # that match their C equivalent are: # &quot;UINT8&quot;; &quot;INT8&quot;; &quot;UINT16&quot;; &quot;INT16&quot;; # &quot;UINT24&quot;; &quot;INT24&quot;; &quot;UINT32&quot;; &quot;INT32&quot;; # &quot;UINT64&quot;; &quot;INT64&quot;; &quot;REAL32&quot;; &quot;REAL64&quot;); attr(ado, &quot;trackFormats&quot;) = c(&quot;REAL32&quot;) # add track ado = addTrack(ado, &quot;sine&quot;, sineWave, &quot;REAL32&quot;) # write AsspDataObj object to file write.AsspDataObj(dobj = ado, file = file.path(tempdir(), &quot;example.sine&quot;)) ## NULL Although somewhat of a generic example, R Example ?? shows how to generate an AsspDataObj from scratch. This approach can, for example, be used to read in signal data produced by other software or signal data acquisition devices. Hence, this approach can be used to import many forms of data into the EMU-SDMS. Appendix @ref(sec:app-chap-wrassp-praatsSigProc} shows an example of how this approach can be used to take advantage of Praat’s signal processing capabilities and integrate its output into the EMU-SDMS. 8.9 Conclusion The wrassp packages enriches the R package landscape by providing functionality for handling speech signal files in most common audio formats and for performing signal analyses common in the phonetic and speech sciences. The EMU-SDMS utilizes the functionality that the wrassp package provides by allowing the user to calculate signals that match the segments of a segment list. This can either be done in real time or by extracting the signals from files. Hence, the wrassp package is an integral part of the EMU-SDMS but can also be used as a standalone package if so desired. Some examples of this chapter are adapted version of examples given in the wrassp_intro vignette of the wrassp package.↩ "],
["chap-emu-webApp.html", "9 The EMU-webApp16 9.1 Main layout 9.2 General usage 9.3 Configuring the EMU-webApp 9.4 Conclusion", " 9 The EMU-webApp16 The EMU-SDMS has a unique approach to its GUI in that it utilizes a web application as its primary GUI. This is known as the EMU-webApp . The EMU-webApp is a fully fledged browser-based labeling and correction tool that offers a multitude of labeling and visualization features. These features include unlimited undo/redo, formant correction capabilities, the ability to snap a preselected boundary to the nearest top/bottom boundary, snap a preselected boundary to the nearest zero crossing, and many more. The web application is able to render everything directly in the user’s browser, including the calculation and rendering of the spectrogram, as it is written entirely using HTML, CSS and JavaScript. This means it can also be used as a standalone labeling application, as it does not require any server-side calculations or rendering. Further, it is designed to interact with any websocket server that implements the EMU-webApp websocket protocol (see Section ??). This enables it to be used as a labeling tool for collaborative annotation efforts. Also, as the EMU-webApp is cached in the user’s browser on the first visit, it does not require any internet connectivity to be able to access the web application unless the user explicitly clears the browser’s cache. The URL of the current live version of the EMU-webApp is: http://ips-lmu.github.io/EMU-webApp/. 9.1 Main layout The main screen of the EMU-webApp can be split into five areas. Figure 9.1 shows a screenshot of the EMU-webApp’s main screen displaying these five areas while displaying a bundle of the ae demo database. This database is served to the EMU-webApp by invoking the serve() command as shown in R Example ??. The left side bar (area marked 1 in Figure 9.1) represents the bundle list side bar which, if connected to a database, displays the currently available bundles grouped by their sessions. The top and bottom menu bars (areas marked 2 and 5 in Figure 9.1) display the currently available menu options, where the bottom menu bar contains the audio navigation and playback controls and also includes a scrollable mini map of the oscillogram. Area 3 of Figure 9.1 displays the signal canvas area currently displaying the oscillogram and the spectrogram. Other signal contours such as formant frequency contours and fundamental frequency contours are also displayed in this area. Area 4 of Figure 9.1 displays the area in which levels containing time information are displayed. It is worth noting that the main screen of the EMU-webApp does not display any levels that do not contain time information. The hierarchical annotation can be displayed and edited by clicking the show hierarchy button in the top menu bar (see Figure 9.6 for an example of how the hierarchy is displayed). # serve ae emuDB to EMU-webApp serve(ae) Figure 9.1: Screenshot of EMU-webApp displaying the ae demo database with overlaid areas of the main screen of the web application (see text). 9.2 General usage This section introduces the labeling mechanics and general labeling workflow of the EMU-webApp. The EMU-webApp makes heavy use of keyboard shortcuts. Is is worth noting that most of the keyboard shortcuts are centered around the WASD keys, which are the navigation shortcut keys (W to zoom in; S to zoom out; A to move left and D to move right). For a full list of the available keyboard shortcuts see the EMU-webApp’s own manual, which can be accessed by clicking the EMU icon on the right hand side of the top menu bar (area 2 in Figure 9.1). 9.2.1 Annotating levels containing time information 9.2.1.1 Boundaries and events The EMU-webApp has slightly different labeling mechanics compared with other annotation software. Compared to the usual click and drag of segment boundaries and event markers, the web application continuously tracks the movement of the mouse in levels containing time information, highlighting the boundary or event marker that is closest to it by coloring it blue. Figure 9.2 displays this automatic boundary preselection. Figure 9.2: Screenshot of segment level as displayed by the EMU-webApp with superimposed mouse cursor displaying the automatic boundary preselection of closest boundary (boundary marked blue). Once a boundary or event is preselected, the user can perform various actions with it. She or he can, for example, grab a preselected boundary or event by holding down the SHIFT key and moving it to the desired position, or delete the current boundary or event by hitting the BACKSPACE key. Other actions that can be performed on preselected boundaries or events are: snap to closest boundary or event in level above (Keyboard Shortcut t), snap to closest boundary or event in level below (Keyboard Shortcut b), and snap to nearest zero crossing (Keyboard Shortcut x). To add a new boundary or event to a level the user initially has to select the desired level she or he wishes to edit. This is achieved either by using the up and down cursor keys or by single-left-clicking on the desired level. The current preselected level is marked in a darker shade of gray, as is displayed in Figure 9.3. Figure 9.3: Screenshot of two levels as displayed by the EMU-webApp, where the lower level is preselected (i.e., marked in a darker shade of gray). To add a boundary to the currently selected level one first has to select a point in time either in the spectrogram or the oscillogram by single-left-clicking on the desired location. Hitting the enter/return key adds a new boundary or event to the preselected level at the selected time point. Selecting a stretch of time in the spectrogram or the oscillogram (left-click-and-drag) and hitting enter will add a segment (not a boundary) to a preselected segment level. 9.2.1.2 Segments and events The EMU-webApp also allows segments and events to be preselected by single-left-clicking the desired item. The web application colors the preselected segments and events yellow to indicate their pre-selection as displayed in Figure 9.4. Figure 9.4: Screenshot of level as displayed by the EMU-webApp, where the /@/ segment is currently preselected as it is marked yellow. As with preselected boundaries or events the user can now perform multiple actions with these preselected items. She or he can, for example, edit the item’s label by hitting the enter/return key (which can also be achieved by double-left-clicking the item). Other actions that can be performed on preselected items are: Select next item in level (keyboard shortcut TAB), Select previous item in level (keyboard shortcut SHIFT plus TAB), Add time to selected item(s) end (keyboard shortcut +), Add time to selected item(s) start (keyboard shortcut SHIFT plus +), Remove time to selected item(s) end (keyboard shortcut -), Remove time to selected item(s) start (keyboard shortcut SHIFT plus -), and Move selected item(s) (hold down ALT Key and drag to desired position). By right-clicking adjacent segment or events (keyboard shortcut SHIFT plus left or right cursor keys), it is possible to select multiple items at once. 9.2.1.3 Parallel labels in segments and events If a level containing time information has multiple attribute definitions (i.e., multiple parallel labels per segment or event) the EMU-webApp automatically displays radio buttons underneath that level (see red square in Figure 9.5) that allow the user to switch between the parallel labels. Figure 9.5 displays a segment level with three attribute definitions. Figure 9.5: Screenshot of segment level with three attribute definitions. The radio buttons that switch between the parallel labels are highlighted by a red square. 9.2.1.4 Legal labels As mentioned in Section 5.2.3.2, an array of so-called legal labels can be defined for every level or, more specifically, for each attribute definition. The EMU-webApp enforces these legal labels by not allowing any other labels to be entered in the label editing text fields. If an illegal label is entered, the text field will turn red and the EMU-webApp will not permit this label to be saved. 9.2.2 Working with hierarchical annotations17 9.2.2.1 Viewing the hierarchy As mentioned in Section 9.1, pressing the show hierarchy button (keyboard shortcut h) in the top menu bar opens the hierarchy view modal window. As with most modal windows in the EMU-webApp, it can be closed by clicking on the close button, clicking the X circle icon in the top right hand corner of the modal or by hitting the ESCAPE key. By default, the hierarchy modal window displays a horizontal version of the hierarchy for a spatially economical visualization. As most people are more familiar with a vertical hierarchical annotation display, the hierarchy can be rotated by hitting the rotate by 90° button (keyboard shortcut r). Zooming in and out of the hierarchy can be achieved by using the mouse wheel, and moving through the hierarchy in time can be achieved by holding down the left mouse button and dragging the hierarchy in the desired direction. Figure 9.6 shows the hierarchy modal window displaying the hierarchical annotation of a single path (Utterance -&gt; Intonational -&gt; Intermediate -&gt; Word -&gt; Syllable -&gt; Phoneme -&gt; Phonetic) through a multi-path hierarchy of the ae emuDB in its horizontal form. Figure 9.6: Screenshot of the hierarchy modal window level displaying a path through the hierarchy of the ae emuDB in its horizontal form. 9.2.2.2 Selecting a path through the hierarchy As more complex databases have multiple hierarchical paths through their hierarchical annotation structure (see Figure 4.2 for an example of a multi-dimensional hierarchical annotation structure), the hierarchy modal offers a drop-down menu to choose the current path to be displayed. Area 2 in Figure 9.7 marks the hierarchy path drop-down menu of the hierarchy modal. Figure 9.7: Screenshot of top of hierarchy modal window of the EMU-webApp in which the area marked 1 shows the drop-down menus for selecting the parallel label for each level and area 2 marks the hierarchy path drop-down menu. It is worth noting that only non-partial paths can be selected in the hierarchy path drop-down menu. 9.2.2.3 Selecting parallel labels in timeless levels As timeless levels may also contain multiple parallel labels, the hierarchy path modal window provides a drop-down menu for each level to select which label or attribute definition is to be displayed. Area 1 of Figure 9.7 displays these drop-down menus. 9.2.2.4 Adding a new item The hierarchy modal window provides two methods for adding new annotation items to a level. This can either be achieved by pressing the blue and white + button next to the level’s name (which appends a new item to the end of the level) or by preselecting an annotation item (by hovering the mouse over it) and hitting either the n (insert new item before preselected item) or the m key (insert new item after preselected item). 9.2.2.5 Modifying an annotation item An item’s context menu18 is opened by single-left-clicking its node. The resulting context menu displays a text area in which the label of the annotation item can be edited, a play button to play the audio section associated with the item and a collapse arrow button allowing the user to collapse the sub-tree beneath the current item. Collapsing a sub-tree can be useful for masking parts of the hierarchy while editing. A screenshot of the context menu is displayed in Figure 9.8. Figure 9.8: Screenshot of the hierarchy modal window of the EMU-webApp displaying an annotation item’s context menu. 9.2.2.6 Adding a new link Adding a new link between two items can be achieved by hovering the mouse over one of the two items, holding down the SHIFT key and moving the mouse cursor to the other item. A green dashed line indicates that the link to be added is valid, while a red dashed line indicates it is not. A link’s validity is dependent on the database’s configuration (i.e., if there is a link definition present and the type of link definition) as well as the non-crossing constraint (Coleman and Local 1991) that essentially implies that links are not allowed to cross each other. If the link is valid (i.e., a green dashed line is present), releasing the SHIFT key will add the link to the annotation. 9.2.2.7 Deleting an annotation item or a link Items and links are deleted by initially preselecting them by hovering the mouse cursor over them. The preselected items are marked blue and preselected links yellow. A preselected link is removed by hitting BACKSPACE and a preselected item is deleted by hitting the y key. Deleting an item will also delete all links leading to and from it. 9.3 Configuring the EMU-webApp This section will give an overview of how the EMU-webApp can be configured. The configuration of the EMU-webApp is stored in the EMUwebAppConfig section of the _DBconfig.json of an emuDB (see Appendix ?? for details). This means that the EMU-webApp can be configured separately for every emuDB. Although it can be necessary for some advanced configuration options to manually edit the _DBconfig.json using a text editor (see Section 9.3.3), the most common configuration operations can be achieved using functions provided by the emuR package (see Section 9.3.1). A central concept for configuring the EMU-webApp are so-called perspectives. Essentially, a perspective is an independent configuration of how the EMU-webApp displays a certain set of data. Having multiple perspectives allows the user to switch between different views of the data. This can be especially useful when dealing with complex annotations where only showing certain elements for certain labeling tasks can be beneficial. Figure 9.9 displays a screenshot of the perspectives side bar menu of the EMU-webApp which displays the three perspectives of the ae emuDB. The default perspective displays both the Phonetic and the Tone levels where as the Phonetic-only and the Tone-only only display these levels individually. Figure 9.9: Screenshot of the hierarchy modal window of the EMU-webApp displaying an annotation item’s context menu. 9.3.1 Basic configurations using emuR R Example ?? shows how to create and load the demo data that will be used throughout the rest of this chapter. # load package library(emuR) # create demo data in directory provided by tempdir() create_emuRdemoData(dir = tempdir()) # create path to demo database path2ae = file.path(tempdir(), &quot;emuR_demoData&quot;, &quot;ae_emuDB&quot;) # load database ae = load_emuDB(path2ae, verbose = F) As mentioned above, the EMU-webApp subdivides different ways to look at an emuDB into so-called perspectives. Users can switch between these perspectives in the web application. They contain, for example, information on what levels are displayed, which SSFF tracks are drawn. R Example ?? shows how the current perspectives can be listed using the list_perspectives() function. # list perspectives of ae emuDB list_perspectives(ae) ## name signalCanvasesOrder levelCanvasesOrder ## 1 default OSCI; SPEC Phonetic; Tone ## 2 Phonetic-only OSCI; SPEC Phonetic ## 3 Tone-only OSCI; SPEC Tone As it is sometimes necessary to add new or remove existing perspectives to or from a database, R Example ?? shows how this can be achieved using emuR’s add/remove_perspective() functions. # add new perspective to ae emuDB add_perspective(ae, name = &quot;tmpPersp&quot;) # show added perspective list_perspectives(ae) ## name signalCanvasesOrder levelCanvasesOrder ## 1 default OSCI; SPEC Phonetic; Tone ## 2 Phonetic-only OSCI; SPEC Phonetic ## 3 Tone-only OSCI; SPEC Tone ## 4 tmpPersp OSCI; SPEC # remove newly added perspective remove_perspective(ae, name = &quot;tmpPersp&quot;) 9.3.2 Signal canvas and level canvas order As mentioned above, R Example ?? shows that the ae emuDB contains three perspectives. The first perspective (default) displays the oscillogram (OSCI) followed by the spectrogram (SPEC) in the signal canvas area (area 3 of Figure 9.1) and the Phonetic and Tone levels in the level canvas area (area 4 of Figure 9.1). It is worth noting that OSCI (oscillogram) and SPEC (spectrogram) are predefined signal tracks that are always available. This is indicated by the capital letters indicating that they are predefined constants. R Example ?? shows how the order of the signal canvases and level canvases can be changed using the get/set_signalCanvasesOrder() and get/set_levelCanvasesOrder(). # get order vector of signal canvases of default perspective sco = get_signalCanvasesOrder(ae, perspectiveName = &quot;default&quot;) # show sco vector sco ## [1] &quot;OSCI&quot; &quot;SPEC&quot; # reverse sco order # using R&#39;s rev() function scor = rev(sco) # set order vector of signal canvases of default perspective set_signalCanvasesOrder(ae, perspectiveName = &quot;default&quot;, order = scor) # set order vector of level canvases of default perspective # to only display the &quot;Tone&quot; level set_levelCanvasesOrder(ae, perspectiveName = &quot;default&quot;, order = c(&quot;Tone&quot;)) # list perspectives of ae emuDB # to show changes list_perspectives(ae) ## name signalCanvasesOrder levelCanvasesOrder ## 1 default SPEC; OSCI Tone ## 2 Phonetic-only OSCI; SPEC Phonetic ## 3 Tone-only OSCI; SPEC Tone After the changes made in R Example ??, the default perspective will show the spectrogram above the oscillogram in the signal canvas area and only the Tone level in the level canvas area. Only levels with time information are allowed to be displayed in the level canvas area, and the set_levelCanvasesOrder() will print an error if a level of type ITEM is added (see R Example ??). # set level canvas order where a # level is passed into the order parameter # that is not of type EVENT or SEGMENT set_levelCanvasesOrder(ae, perspectiveName = &quot;default&quot;, order = c(&quot;Syllable&quot;)) ## Error in set_levelCanvasesOrder(ae, perspectiveName = &quot;default&quot;, order = c(&quot;Syllable&quot;)): levelDefinition with name &#39;Syllable&#39; is not of type &#39;SEGMENT&#39; or &#39;EVENT&#39; The same mechanism used above can also be used to display any SSFF track that is defined for the database by referencing its name. R Example ?? shows how the existing SSFF track called fm (containing formant values calculated by wrassp’s forest() function) can be added to the signal canvas area. # show currently available SSFF tracks list_ssffTrackDefinitions(ae) ## name columnName fileExtension ## 1 dft dft dft ## 2 fm fm fms # re-set order vector of signal canvases of default perspective # by appending the fm track set_signalCanvasesOrder(ae, perspectiveName = &quot;default&quot;, order = c(scor, &quot;fm&quot;)) A screenshot of the current display of the default perspective can be seen in Figure 9.10. Figure 9.10: Screenshot of signal and level canvases displays of the EMU-webApp after the changes made in R Examples (???)(rexample:webApp-oders) and (???)(rexample:webApp-addFormantsToSignalCanvases). 9.3.3 Advanced configurations made by editing the _DBconfig.json Although the above configuration options cover the most common use cases, the EMU-webApp offers multiple other configuration options that are currently not configurable via functions provided by emuR. These advanced configuration options can currently only be achieved by manually editing the _DBconfig.json file using a text editor. As even the colors used in the EMU-webApp and every keyboard shortcut can be reconfigured, here we will focus on the more common advanced configuration options. A full list of the available configuration fields of the EMUwebAppConfig section of the _DBconfig.json including their meaning, can be found in Appendix ??. 9.3.3.1 Overlaying signal canvases To save space it can be beneficial to overlay one or more signal tracks onto other signal canvases. This can be achieved by manually editing the assign array of the EMUwebAppConfig:perspectives[persp_idx]:signalCanvases field in the _DBconfig.json. Listing ?? shows an example configuration that overlays the fm track on the oscillogram where the OSCI string can be replaced by any other entry in the EMUwebAppConfig:perspectives[persp_idx]:signalCanvases:order array. Figure 9.11 displays a screenshot of such an overlay. ... &quot;assign&quot;: [{ &quot;signalCanvasName&quot;: &quot;OSCI&quot;, &quot;ssffTrackName&quot;: &quot;fm&quot; }], ... Figure 9.11: Screenshot of signal canvases display of the EMU-webApp after the changes made in R Examples (???)(rexample:webApp-oders) and (???)(rexample:webApp-addFormantsToSignalCanvases). 9.3.3.2 Frequency-aligned formant contours spectrogram overlay The current mechanism for laying frequency-aligned formant contours over the spectrogram is to give the formant track the predefined name FORMANTS. If the formant track is called FORMANTS and it is assigned to be laid over the spectrogram (see Listing ??) the EMU-webApp will frequency-align the contours to the current minimum and maximum spectrogram frequencies (see Figure 9.12). ... &quot;assign&quot;: [{ &quot;signalCanvasName&quot;: &quot;SPEC&quot;, &quot;ssffTrackName&quot;: &quot;FORMANTS&quot; }], ... Figure 9.12: Screenshot of signal canvases area of the EMU-webApp displaying formant contours that are overlaid on the spectrogram and frequency-aligned. 9.3.3.3 Correcting formants The above configuration of the frequency-aligned formant contours will automatically allow the FORMANTS track to be manually corrected. Formants can be corrected by hitting the appropriate number key (1 = first formant, 2 = second formant, …). Similar to boundaries and events, the mouse cursor will automatically be tracked in the SPEC canvas and the nearest formant value preselected. Holding down the SHIFT key moves the current formant value to the mouse position, hence allowing the contour to be redrawn and corrected. 9.3.4 2D canvas The EMU-webApp has an additional canvas which can be configured to display two-dimensional data. Figure 9.13 shows a screenshot of the 2D canvas, which is placed in the bottom right hand corner of the level canvas area of the web application. The screenshot shows data representing EMA sensor positions on the mid sagittal plane. Listings ?? shows how the 2D canvas can be configured. Essentially, every drawn dot is configured by assigning a column in an SSFF track that specifies the X values and an additional column that specifies the Y values. Figure 9.13: Screenshot of 2D canvas of the EMU-webApp displaying two-dimensional EMA data. ... &quot;twoDimCanvases&quot;: { &quot;order&quot;: [&quot;DOTS&quot;], &quot;twoDimDrawingDefinitions&quot;: [{ &quot;name&quot;: &quot;DOTS&quot;, &quot;dots&quot;: [{ &quot;name&quot;: &quot;tt&quot;, &quot;xSsffTrack&quot;: &quot;tt_posy&quot;, &quot;xContourNr&quot;: 0, &quot;ySsffTrack&quot;: &quot;tt_posz&quot;, &quot;yContourNr&quot;: 0, &quot;color&quot;: &quot;rgb(255,0,0)&quot; }, ... &quot;connectLines&quot;: [{ &quot;fromDot&quot;: &quot;tt&quot;, &quot;toDot&quot;: &quot;tm&quot;, &quot;color&quot;: &quot;rgb(0,0,0)&quot; }, ... 9.3.4.1 EPG The 2D canvas of the EMU-webApp can also be configured to display EPG data as displayed in Figure 9.14. The SSFF file containing the EPG data has to be formated in a specific way. The format is a set of eight bytes per point in time, where each byte represents a row of electrodes on the artificial palate. Each binary bit value per byte indicates whether one of the eight sensors is activated or not (i.e., tongue contact was measured). If data in this format and an SSFF track with the predefined name EPG referencing the SSFF files are present, the 2D canvas can be configured to display this data by adding the EPG to the twoDimCanvases:order array as shown in Listing ??. Figure 9.14: Screenshot of 2D canvas of the EMU-webApp displaying EPG palate traces. &quot;twoDimCanvases&quot;: { &quot;order&quot;: [&quot;EPG&quot;] } 9.3.4.2 EMA gestural landmark recognition The EMU-webApp can also be configured to semi-automatically detect gestural landmarks of EMA contours. The functions implemented in the EMU-webApp are based on various Matlab scripts by Phil Hoole. For a description of which gestural landmarks are detected and how these are detected, see Bombien (2011) page 61 ff. Compared to the above configurations, configuring the EMU-webApp to semi-automatically detect gestural landmarks of EMA contours is done as part of the level definition’s configuration entries of the _DBconfig.json. Listing ?? shows the anagestConfig entry, which configures the tongueTipGestures event level for this purpose. Within the web application this level has to be preselected by the user and a region containing a gesture in the SSFF track selected (left click and drag). Hitting the ENTER/RETURN key then executes the semi-automatic gestural landmark recognition functions. If multiple candidates are recognized for certain landmarks, the user will be prompted to select the appropriate landmark. ... &quot;levelDefinitions&quot;: [{ { &quot;name&quot;: &quot;tongueTipGestures&quot;, &quot;type&quot;: &quot;EVENT&quot;, &quot;attributeDefinitions&quot;: [{ &quot;name&quot;: &quot;tongueTipGestures&quot;, &quot;type&quot;: &quot;STRING&quot; }], &quot;anagestConfig&quot;: { &quot;verticalPosSsffTrackName&quot;: &quot;tt_posz&quot;, &quot;velocitySsffTrackName&quot;: &quot;t_tipTV&quot;, &quot;autoLinkLevelName&quot;: &quot;ORT&quot;, &quot;multiplicationFactor&quot;: 1, &quot;threshold&quot;: 0.2, &quot;gestureOnOffsetLabels&quot;: [&quot;gon&quot;, &quot;goff&quot;], &quot;maxVelocityOnOffsetLabels&quot;: [&quot;von&quot;, &quot;voff&quot;], &quot;constrictionPlateauBeginEndLabels&quot;: [&quot;pon&quot;, &quot;poff&quot;], &quot;maxConstrictionLabel&quot;: &quot;mon&quot; } ... The user will be prompted to select an annotation item of the level specified in anagestConfig:autoLinkLevelName once the gestural landmarks are recognized. The EMU-webApp then automatically links all gestural landmark events to that item. 9.4 Conclusion This chapter provided an overview of the EMU-webApp by showing the main layout and configuration options and how its labeling mechanics work. To our knowledge, the EMU-webApp is the first client-side web-based annotation tool that is this feature rich. Being completely web-based not only allows it to be used within the context of the EMU-SDMS but also allows it to connect to any web server that implements the EMU-webApp-websocket-protocol (see Appendix ?? for details). This feature is currently being utilized, for example, by the IPS-EMUprot-nodeWSserver.js server side software package (see https://github.com/IPS-LMU/IPS-EMUprot-nodeWSserver), which allows emuDBs to be served to any number of clients for collaborative annotation efforts. Further, by using the URL Parameters (see Chapter ?? for details) the web application can also be used to display annotation data that is hosted on any web server.19 Because of these features, we feel the EMU-webApp is a valuable contribution to the speech and spoken language software tool landscape. References "],
["references.html", "References", " References "]
]
